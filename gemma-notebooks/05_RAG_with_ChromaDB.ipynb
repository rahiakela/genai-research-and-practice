{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/gemma-notebooks/05_RAG_with_ChromaDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsDR_omdNea"
      },
      "source": [
        "# Gemma - RAG with ChromaDB\n",
        "\n",
        "This cookbook demonstrates how you can build a minimal-ish Retrieval-Augmented Generation (RAG) system without using any orchestration tool like LangChain or LlamaIndex. It only uses [ChromaDB](https://www.trychroma.com/) as the vector database for storing and querying embeddings.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/RAG_with_ChromaDB.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "### Select the Colab runtime\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model. In this case, you can use a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **▾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "\n",
        "### Gemma setup on Hugging Face\n",
        "This cookbook uses the instruction tuned Gemma 7B model through Hugging Face. So you will need to:\n",
        "\n",
        "* Get access to Gemma on [huggingface.co](huggingface.co) by accepting the Gemma license on the Hugging Face page of the specific model, i.e., [Gemma 7B IT](https://huggingface.co/google/gemma-7b-it).\n",
        "* Generate a [Hugging Face access token](https://huggingface.co/docs/hub/en/security-tokens) and configure it as a Colab secret 'HF_TOKEN'."
      ],
      "metadata": {
        "id": "MwMiP7jDdAL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "Large Language Models (LLMs) can learn new abilities without directly being trained on them. However, LLMs have been known to \"hallucinate\" when tasked with providing responses for questions they have not been trained on. This is partly because LLMs are unaware of events after training. It is also very difficult to trace the sources from which LLMs draw their responses from. For reliable, scalable applications, it is important that an LLM provides responses that are grounded in facts and is able to cite its information sources.\n",
        "\n",
        "A common approach used to overcome these constraints is called Retrieval Augmented Generation (RAG), which augments the prompt sent to an LLM with relevant data retrieved from an external knowledge base through an Information Retrieval (IR) mechanism. The knowledge base can be your own corpora of documents, databases, or APIs.\n",
        "\n",
        "### Chunking the data\n",
        "\n",
        "To improve the relevance of content returned by the vector database during  retrieval, break down large documents into smaller pieces or chunks while ingesting the document.\n",
        "\n",
        "In this cookcook, you will use the [Google I/O 2024 Gemma family expansion launch blog](https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/) as the sample document and Google's [Open Source HtmlChunker](https://github.com/google/labs-prototypes/tree/main/seeds/chunker-python) to chunk it up into passages."
      ],
      "metadata": {
        "id": "SGDV_qlIOu2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-labs-html-chunker\n",
        "\n",
        "from google_labs_html_chunker.html_chunker import HtmlChunker\n",
        "\n",
        "from urllib.request import urlopen"
      ],
      "metadata": {
        "id": "pdWNUmA_zcnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with urlopen(\n",
        "    \"https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/\"\n",
        ") as f:\n",
        "    html = f.read().decode(\"utf-8\")\n",
        "\n",
        "# Chunk the file using HtmlChunker\n",
        "chunker = HtmlChunker(\n",
        "    max_words_per_aggregate_passage=200,\n",
        "    greedily_aggregate_sibling_nodes=True,\n",
        "    html_tags_to_exclude={\"noscript\", \"script\", \"style\"},\n",
        ")\n",
        "passages = chunker.chunk(html)"
      ],
      "metadata": {
        "id": "S6ev2LijJ1Nu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at how the chunked text look like."
      ],
      "metadata": {
        "id": "myiMmXpFQDSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for passage in passages:\n",
        "    print(passage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N63HefX-pYa",
        "outputId": "f5f53197-70ef-4b34-d7ad-eacf092313b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introducing PaliGemma, Gemma 2, and an Upgraded Responsible AI Toolkit\n",
            "            \n",
            "            \n",
            "            \n",
            "            - Google Developers Blog\n",
            "Products Develop Android Chrome ChromeOS Cloud Firebase Flutter Google Assistant Google Maps Platform Google Workspace TensorFlow YouTube Grow Firebase Google Ads Google Analytics Google Play Search Web Push and Notification APIs Earn AdMob Google Ads API Google Pay Google Play Billing Interactive Media Ads Solutions Events Learn Community Groups Google Developer Groups Google Developer Student Clubs Woman Techmakers Google Developer Experts Tech Equity Collective Programs Accelerator Solution Challenge DevFest Stories All Stories Developer Program Blog Search English English Español (Latam) Bahasa Indonesia 日本語 한국어 Português (Brasil) 简体中文\n",
            "Products More Solutions Events Learn Community More Developer Program Blog Develop Android Chrome ChromeOS Cloud Firebase Flutter Google Assistant Google Maps Platform Google Workspace TensorFlow YouTube Grow Firebase Google Ads Google Analytics Google Play Search Web Push and Notification APIs Earn AdMob Google Ads API Google Pay Google Play Billing Interactive Media Ads Groups Google Developer Groups Google Developer Student Clubs Woman Techmakers Google Developer Experts Tech Equity Collective Programs Accelerator Solution Challenge DevFest Stories All Stories English Español (Latam) Bahasa Indonesia 日本語 한국어 Português (Brasil) 简体中文\n",
            "Gemini Introducing PaliGemma, Gemma 2, and an Upgraded Responsible AI Toolkit MAY 14, 2024 Tris Warkentin Director, Product Management Xiaohua Zhai Senior Staff Research Scientist Ludovic Peran Product Manager Share Facebook Twitter LinkedIn Mail\n",
            "At Google, we believe in the power of collaboration and open research to drive innovation, and we're grateful to see Gemma embraced by the community with millions of downloads within a few short months of its launch. This enthusiastic response has been incredibly inspiring, as developers have created a diverse range of projects like Navarasa , a multilingual variant for Indic languages, to Octopus v2 , an on-device action model, developers are showcasing the potential of Gemma to create impactful and accessible AI solutions. This spirit of exploration and creativity has also fueled our development of CodeGemma , with its powerful code completion and generation capabilities, and RecurrentGemma , offering efficient inference and research possibilities.\n",
            "Link to Youtube Video (visible only when JS is disabled)\n",
            "Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Today, we're excited to further expand the Gemma family with the introduction of PaliGemma , a powerful open vision-language model (VLM), and a sneak peek into the near future with the announcement of Gemma 2. Additionally, we're furthering our commitment to responsible AI with updates to our Responsible Generative AI Toolkit, providing developers with new and enhanced tools for evaluating model safety and filtering harmful content.\n",
            "Introducing PaliGemma: Open Vision-Language Model PaliGemma is a powerful open VLM inspired by PaLI-3 . Built on open components including the SigLIP vision model and the Gemma language model, PaliGemma is designed for class-leading fine-tune performance on a wide range of vision-language tasks. This includes image and short video captioning, visual question answering, understanding text in images, object detection, and object segmentation. We're providing both pretrained and fine-tuned checkpoints at multiple resolutions, as well as checkpoints specifically tuned to a mixture of tasks for immediate exploration. To facilitate open exploration and research, PaliGemma is available through various platforms and resources. Start exploring today with free options like Kaggle and Colab notebooks. Academic researchers seeking to push the boundaries of vision-language research can also apply for Google Cloud credits to support their work. Get started with PaliGemma today. You can find PaliGemma on GitHub, Hugging Face models , Kaggle, Vertex AI Model Garden , and ai.nvidia.com (accelerated with TensoRT-LLM) with easy integration through JAX and Hugging Face Transformers. (Keras integration coming soon) You can also interact with the model via this Hugging Face Space .\n",
            "Screenshot from the HuggingFace Space running PaliGemma\n",
            "Announcing Gemma 2: Next-Gen Performance and Efficiency We're thrilled to announce the upcoming arrival of Gemma 2, the next generation of Gemma models. Gemma 2 will be available in new sizes for a broad range of AI developer use cases and features a brand new architecture designed for breakthrough performance and efficiency, offering benefits such as: Class Leading Performance: At 27 billion parameters, Gemma 2 delivers performance comparable to Llama 3 70B at less than half the size. This breakthrough efficiency sets a new standard in the open model landscape. Reduced Deployment Costs : Gemma 2's efficient design allows it to fit on less than half the compute of comparable models. The 27B model is optimized to run on NVIDIA’s GPUs or can run efficiently on a single TPU host in Vertex AI, making deployment more accessible and cost-effective for a wider range of users.\n",
            "Versatile Tuning Toolchains: Gemma 2 will provide developers with robust tuning capabilities across a diverse ecosystem of platforms and tools. From cloud-based solutions like Google Cloud to popular community tools like Axolotl , fine-tuning Gemma 2 will be easier than ever. Plus, seamless partner integration with Hugging Face and NVIDIA TensorRT-LLM, along with our own JAX and Keras, ensures you can optimize performance and efficiently deploy across various hardware configurations.\n",
            "Gemma 2 is still pretraining. This chart shows performance from the latest Gemma 2 checkpoint along with benchmark pretraining metrics. Source: Hugging Face Open LLM Leaderboard (April 22, 2024) and Grok announcement blog\n",
            "Stay tuned for the official launch of Gemma 2 in the coming weeks! Expanding the Responsible Generative AI Toolkit For this reason we're expanding our Responsible Generative AI Toolkit to help developers conduct more robust model evaluations by releasing the LLM Comparator in open source. The LLM Comparator is a new interactive and visual tool to perform effective side-by-side evaluations to assess the quality and safety of model' responses. To see the LLM Comparator in action, explore our demo showcasing a comparison between Gemma 1.1 and Gemma 1.0.\n",
            "We hope that this tool will advance further the toolkit’s mission to help developers create AI applications that are not only innovative but also safe and responsible. As we continue to expand the Gemma family of open models, we remain dedicated to fostering a collaborative environment where cutting-edge AI technology and responsible development go hand in hand. We're excited to see what you build with these new tools and how, together, we can shape the future of AI.\n",
            "posted in: Gemini AI Announcements Explore Learn Previous Next Related Posts Mobile Web Announcements Bringing the I/O magic to Berlin June 27, 2024 Gemma AI Announcements Fine-tuning Gemma 2 with Keras - and an update from Hugging Face June 27, 2024 Firebase Flutter AI How-To Guides Industry Trends How We Built It: The I/O Crossword June 26, 2024 Gemini AI Announcements Industry Trends Gemini 1.5 Pro 2M context window, code execution capabilities, and Gemma 2 are available today June 27, 2024\n",
            "Connect Blog Instagram LinkedIn Twitter YouTube Programs Women Techmakers Google Developer Groups Google Developer Experts Accelerators Google Developer Student Clubs Developer consoles Google API Console Google Cloud Platform Console Google Play Console Firebase Console Actions on Google Console Cast SDK Developer Console Chrome Web Store Dashboard\n",
            "Android Chrome Firebase Google Cloud Platform All products Manage cookies Terms Privacy English English Español (Latam) Bahasa Indonesia 日本語 한국어 Português (Brasil) 简体中文\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index the chunks with a vector database"
      ],
      "metadata": {
        "id": "iLBZLeZpQL5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will now use ChromaDB, an open source embedding database, to index the passages."
      ],
      "metadata": {
        "id": "19yZsnG1RoTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "QY9gfGPH2a6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client = chromadb.Client()\n",
        "collection = chroma_client.create_collection(name=\"cookbook_collection\")\n",
        "collection.add(documents=passages, ids=[str(i) for i in range(len(passages))])"
      ],
      "metadata": {
        "id": "GJaFfs30KQPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you retrieve relevant passages from the vector database as the context, based on the user question and assemble a prompt using both the user question and retrieved context."
      ],
      "metadata": {
        "id": "CqXD_0NyUEtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"You are an expert in answering user questions. You always understand user questions well, and then provide high-quality answers based on the information provided in the context.\n",
        "\n",
        "If the provided context does not contain relevent information, just respond \"I could not find the answer based on the context you provided.\"\n",
        "\n",
        "User question: {}\n",
        "\n",
        "Context:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "user_question = \"how many parameters does Gemma 2 have?\"\n",
        "\n",
        "results = collection.query(query_texts=user_question, n_results=3)\n",
        "\n",
        "context = \"\\n\".join(\n",
        "    [f\"{i+1}. {passage}\" for i, passage in enumerate(results[\"documents\"][0])]\n",
        ")\n",
        "prompt = f\"{prompt_template.format(user_question, context)}\""
      ],
      "metadata": {
        "id": "ODgapzEc8ir-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the final prompt that will be sent to Gemma."
      ],
      "metadata": {
        "id": "8spJD5iwUt0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgh36K4TB5Qf",
        "outputId": "ab9ffeca-f03c-483d-d7b7-d2059b834aeb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an expert in answering user questions. You always understand user questions well, and then provide high-quality answers based on the information provided in the context.\n",
            "\n",
            "If the provided context does not contain relevent information, just respond \"I could not find the answer based on the context you provided.\"\n",
            "\n",
            "User question: how many parameters does Gemma 2 have?\n",
            "\n",
            "Context:\n",
            "1. Gemma 2 is still pretraining. This chart shows performance from the latest Gemma 2 checkpoint along with benchmark pretraining metrics. Source: Hugging Face Open LLM Leaderboard (April 22, 2024) and Grok announcement blog\n",
            "2. Stay tuned for the official launch of Gemma 2 in the coming weeks! Expanding the Responsible Generative AI Toolkit For this reason we're expanding our Responsible Generative AI Toolkit to help developers conduct more robust model evaluations by releasing the LLM Comparator in open source. The LLM Comparator is a new interactive and visual tool to perform effective side-by-side evaluations to assess the quality and safety of model' responses. To see the LLM Comparator in action, explore our demo showcasing a comparison between Gemma 1.1 and Gemma 1.0.\n",
            "3. Announcing Gemma 2: Next-Gen Performance and Efficiency We're thrilled to announce the upcoming arrival of Gemma 2, the next generation of Gemma models. Gemma 2 will be available in new sizes for a broad range of AI developer use cases and features a brand new architecture designed for breakthrough performance and efficiency, offering benefits such as: Class Leading Performance: At 27 billion parameters, Gemma 2 delivers performance comparable to Llama 3 70B at less than half the size. This breakthrough efficiency sets a new standard in the open model landscape. Reduced Deployment Costs : Gemma 2's efficient design allows it to fit on less than half the compute of comparable models. The 27B model is optimized to run on NVIDIA’s GPUs or can run efficiently on a single TPU host in Vertex AI, making deployment more accessible and cost-effective for a wider range of users.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate the answer\n",
        "\n",
        "Now load the Gemma model in quanzied 4-bit mode using Hugging Face."
      ],
      "metadata": {
        "id": "uXLpmtoeU0gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes accelerate\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "import bitsandbytes, accelerate"
      ],
      "metadata": {
        "id": "ebiyZjTTL9Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"google/gemma-1.1-7b-it\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"quantization_config\": {\"load_in_4bit\": True},\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "LlQ0qLieCWCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, generate the answer."
      ],
      "metadata": {
        "id": "gsmvKH-UU8yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "prompt = pipeline.tokenizer.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.1)\n",
        "print(outputs[0][\"generated_text\"][len(prompt) :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxPUcDSHDHpw",
        "outputId": "a22ee78e-72c7-4465-9b9e-d725f53808c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma 2 has **27 billion parameters**.\n",
            "\n",
            "The context explicitly states that Gemma 2 has 27 billion parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemma is able to provide the correct answer based on the context."
      ],
      "metadata": {
        "id": "3s9cjDsMWP9g"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}