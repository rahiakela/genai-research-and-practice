{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMyFExvbJZralHKX2b4Hy4k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/generative-ai-with-langchain/01_langchain_with_hugging_face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "zE2_Qyu_QVFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "XbBWGhlzQV3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "y5NGphhDQjUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hugging Face"
      ],
      "metadata": {
        "id": "GiVdWV5MRS-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_text_model = pipeline(\n",
        "    model=\"aisquared/dlite-v1-355m\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    framework=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "L0a0fuIIRPE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_text_model(\"In this chapter, we'll discuss first steps with generative AI in Python.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "jdOGcaSwRl3b",
        "outputId": "48630577-f5a7-4648-d11a-602224b0d9be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Steps with Generative AI in Python:\\n1. Set up a Python environment:\\n\\nYou'll need to create a Python environment that allows you to run Python programs. This can be done through a command line interface.\\n\\n2. Install generative models:\\n\\nGenerative models provide a way to generate unseen data from a given input. To install them, first get them and then download them.\\n\\n3. Create a data set:\\n\\nGenerative models are composed of hidden layers of labeled data which are graphically represented in Python objects. To create a data set, first draw a line down the middle of the graph of objects. Afterwards, fill in the remaining areas with random data.\\n\\n4. Iterate through data:\\n\\nGenerative models are iteratively generated data sets, which iterate through each field of the data set until it reaches a final iteration. You can inspect the data elements in a given step to get an overview of the iteration.\\n\\n5. Create a decision tree:\\n\\nDecision trees are tree-like trees in which each node represents a decision and the relationship between different entities is well-defined. To create a decision tree, first draw a tree branch with the given decision node, then\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LangChain"
      ],
      "metadata": {
        "id": "_E9kqk9XR0xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
        ")"
      ],
      "metadata": {
        "id": "-alufHztXHmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_template = \"\"\"\n",
        "Question: {question}\n",
        "Answer: Let's think step by step.\n",
        "\"\"\"\n",
        "\n",
        "q_prompt = PromptTemplate.from_template(q_template)\n",
        "chain = q_prompt | hf_llm"
      ],
      "metadata": {
        "id": "Tv177dPvRu94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is electroencephalography?\"\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdwkRsLiSaVr",
        "outputId": "939777a1-5217-41f8-bcb9-dbf537c7e731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First, I'll start by saying that this question (by means of a definition and a discussion of the relation of \"psychical information\" with general cognitive functions) requires that we understand how \"psychical information\" has any relation to cognition. So let's say we have some data: We are studying a school teacher who is trying to convince us that the girl in the picture is not happy or that her face is looking weird.\n",
            "The problem for us is that the \"psychical information\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is philosophy?\"\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqXbLX3qZ17c",
        "outputId": "1892c554-ec65-4abd-9b9b-1f97925afbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(The following question is for use by philosophers, so all of them have to answer the question at hand. They can't just answer, but their understanding of this question has to be perfect and their thinking ought to follow those of the philosophers who speak of it.)\n",
            "Some philosophy is simply the process of acquiring and acquiring an object, some philosophy is the process of trying to find it.\n",
            "There are quite a few philosophical problems of the kind that need explaining.\n",
            "1. The philosopher tries on\n"
          ]
        }
      ]
    }
  ]
}