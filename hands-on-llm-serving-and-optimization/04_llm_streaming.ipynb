{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/hands-on-llm-serving-and-optimization/04_llm_streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "7MInbXv32n97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet vllm transformers tiktoken"
      ],
      "metadata": {
        "id": "1pTMFepDwNZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from vllm.engine.arg_utils import AsyncEngineArgs\n",
        "from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
        "from vllm.sampling_params import SamplingParams"
      ],
      "metadata": {
        "id": "yKM1Wmh41Sx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "4Iucg37wQhNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the engine arguments\n",
        "engine_args = AsyncEngineArgs(\n",
        "    model=\"Qwen/Qwen2.5-0.5B\",\n",
        "    dtype=\"float16\",\n",
        "    tensor_parallel_size=1,      # Number of GPUs to use\n",
        "    gpu_memory_utilization=0.9,  # GPU memory utilization\n",
        "    max_num_batched_tokens=32768,# Maximum number of tokens to process in a batch\n",
        "    max_num_seqs=256,            # Maximum number of sequences to process\n",
        "    disable_log_requests=True,   # Disable request logging\n",
        "    disable_log_stats=True,      # Disable stats logging\n",
        ")\n",
        "\n",
        "# Create the vLLM async streaming engine\n",
        "engine = AsyncLLMEngine.from_engine_args(engine_args)\n"
      ],
      "metadata": {
        "id": "PCNhqxbjQ_FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Streaming"
      ],
      "metadata": {
        "id": "p4hpCCTICXg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming:\n",
        "1. Return result when generation completes.\n",
        "2. Return as soon as we have a token."
      ],
      "metadata": {
        "id": "EjL8QT1cCffP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mention this code is just show, please run streaming.py for actual execution."
      ],
      "metadata": {
        "id": "5RzJ9SwpOVUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sudo code, please run stream.py code to see the execution."
      ],
      "metadata": {
        "id": "Dwmpc7F0RdAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt.\n",
        "prompt = \"\"\"You are an expert AI historian writing a detailed chapter for a book titled \"The Evolution of Human-AI Collaboration.\"\n",
        "\n",
        "Begin by summarizing the early stages of artificial intelligence in the 1950s, touching on symbolic logic and rule-based systems. Then transition into the rise of machine learning, particularly deep learning in the 2010s.\n",
        "\n",
        "Afterward, describe how large language models like GPT transformed human-computer interaction, enabling applications in education, creative writing, customer support, and software development.\n",
        "\n",
        "Finally, reflect on the societal and ethical implications of AI, such as misinformation, bias, and the alignment problem.\n",
        "\n",
        "Write in a formal tone, with rich detail and examples in each era.\"\"\""
      ],
      "metadata": {
        "id": "AOzMRkLICC9z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def generate_text(prompt: str, max_tokens: int = 100, request_id=\"id\"):\n",
        "  try:\n",
        "    # Define sampling parameters\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.0,\n",
        "        max_tokens=max_tokens,\n",
        "        stop=[\"\\n\"],  # Stop at newline\n",
        "    )\n",
        "\n",
        "    # Generate text in async and streaming fashion\n",
        "    results_generator = engine.generate(\n",
        "        prompt=prompt,\n",
        "        sampling_params=sampling_params,\n",
        "        request_id=request_id\n",
        "    )\n",
        "\n",
        "    # Process the results\n",
        "    final_output = None\n",
        "    async for request_output in results_generator:\n",
        "        final_output = request_output\n",
        "        # Print each token as it's generated\n",
        "        print(\"chunk \\n\")\n",
        "        for output in request_output.outputs:\n",
        "            print(output.text, end=\"\", flush=True)\n",
        "        print()\n",
        "    print()  # Newline at the end\n",
        "\n",
        "    # This will only be reached if all tokens are generated\n",
        "    print(\"\\nGeneration completed successfully\")\n",
        "\n",
        "    return final_output\n",
        "  except asyncio.CancelledError:\n",
        "    print(\"\\nGeneration was cancelled\")\n",
        "    return None\n",
        "  finally:\n",
        "    # Always clean up\n",
        "    try:\n",
        "      await engine.abort(request_id)\n",
        "    except:\n",
        "      pass"
      ],
      "metadata": {
        "id": "gAme1aaoewHs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of the streamingusage:\n",
        "prompt = \"What is the capital of US?\"\n",
        "asyncio.run(generate_text(prompt)).outputs[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "vLmXasVODmDr",
        "outputId": "3907d608-8024-453d-9bfc-d2c97e631100"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-485958971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example of the streamingusage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What is the capital of US?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of the streamingusage:\n",
        "prompt = \"What is the capital of US?\"\n",
        "res = asyncio.run(generate_text(prompt))\n",
        "print(res.outputs[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "2S9prM4jC87P",
        "outputId": "82738fe6-d576-4bea-c364-b9c7e83bf67d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-1342938440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example of the streamingusage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What is the capital of US?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "request_id = \"any_id\"\n",
        "await generate_text(prompt, 10000, requtest_id)\n"
      ],
      "metadata": {
        "id": "zi65dpiO-NFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have problem running above code due to Jupyter event loop, try run the example (https://github.com/orca3/llm-model-serving/blob/main/ch02/streaming.py) in terminal."
      ],
      "metadata": {
        "id": "TsO77K61pBsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/orca3/llm-model-serving/raw/main/ch02/streaming.py"
      ],
      "metadata": {
        "id": "cL0vsN7vEicd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python streaming.py"
      ],
      "metadata": {
        "id": "4rkaSOT4ElRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1A1Jv7e6Eset"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}