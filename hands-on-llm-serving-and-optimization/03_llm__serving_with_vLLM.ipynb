{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/hands-on-llm-serving-and-optimization/03_llm__serving_with_vLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# Unload models and clean up gpu memory cache\n",
        "def free_gpu(model):\n",
        "  if model:\n",
        "    # Removes the reference to the model's memory,\n",
        "    # making it eligible for garbage collection.\n",
        "    del model\n",
        "\n",
        "  # Release any cached GPU memory that's no longer needed.\n",
        "  if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "  # Trigger garbage collection to ensure memory is fully released.\n",
        "  gc.collect()\n",
        "\n",
        "free_gpu(None)\n"
      ],
      "metadata": {
        "id": "bCLKJ4c60MQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "vwifYf3us5jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Qwen model with vLLM and track the inference time."
      ],
      "metadata": {
        "id": "vREEwmA2m76F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2YbOJ04uVda"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "\n",
        "# Load model with vLLM.\n",
        "llm = LLM(model=model_name, dtype=\"float16\")\n",
        "\n",
        "# Define the prompt.\n",
        "prompt = \"\"\"You are an expert AI historian writing a detailed chapter for a book titled \"The Evolution of Human-AI Collaboration.\"\n",
        "\n",
        "Begin by summarizing the early stages of artificial intelligence in the 1950s, touching on symbolic logic and rule-based systems. Then transition into the rise of machine learning, particularly deep learning in the 2010s.\n",
        "\n",
        "Afterward, describe how large language models like GPT transformed human-computer interaction, enabling applications in education, creative writing, customer support, and software development.\n",
        "\n",
        "Finally, reflect on the societal and ethical implications of AI, such as misinformation, bias, and the alignment problem.\n",
        "\n",
        "Write in a formal tone, with rich detail and examples in each era.\"\"\"\n",
        "\n",
        "# Create sampling parameters.\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=128)\n",
        "\n",
        "# Time the model generation.\n",
        "start_time = time.time()\n",
        "outputs = llm.generate([prompt], sampling_params)\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results.\n",
        "for output in outputs:\n",
        "  print(f\"Generated text: {output}\")\n",
        "  print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "free_gpu(llm)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "free_gpu(llm)"
      ],
      "metadata": {
        "id": "c-biu-0E4dW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Run Qwen model with standard (non-optimial) HuggingFace library and track the inference time."
      ],
      "metadata": {
        "id": "9XQNdUHknIU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Basic Model Serving (transformers) ---\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "start_time_basic = time.time()\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\", device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "# Create the pipeline.\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "outputs_basic = generator(prompt, max_length=128, temperature=0.8, top_p=0.95)\n",
        "end_time_basic = time.time()\n",
        "\n",
        "print(\"\\n---- Basic Model Serving Results ----\")\n",
        "for output in outputs_basic:\n",
        "    print(f\"Generated text: {output['generated_text']}\")\n",
        "    print(f\"Time taken: {end_time_basic - start_time_basic:.2f} seconds\")\n",
        "\n",
        "\n",
        "print(f\"\\nLatency difference: {(end_time_basic - start_time_basic) - (end_time - start_time):.2f} seconds\")\n",
        "\n",
        "free_gpu(generator)\n"
      ],
      "metadata": {
        "id": "XvVSbcZs3POx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nLatency difference: {(end_time_basic - start_time_basic) - (end_time - start_time):.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "h0vy2RHyczoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WHXE_xr9vjVr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}