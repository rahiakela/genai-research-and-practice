{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/hands-on-llm-serving-and-optimization/02_llm_execution_workthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers tiktoken transformers_stream_generator bertviz"
      ],
      "metadata": {
        "id": "IXrK7CicSXif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# Unload models and clean up gpu memory cache\n",
        "def free_gpu(model):\n",
        "  if model:\n",
        "    # Removes the reference to the model's memory,\n",
        "    # making it eligible for garbage collection.\n",
        "    del model\n",
        "\n",
        "  # Release any cached GPU memory that's no longer needed.\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Trigger garbage collection to ensure memory is fully released.\n",
        "  gc.collect()\n"
      ],
      "metadata": {
        "id": "n61-c_bWQ7e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run LLM with HuggingFace library"
      ],
      "metadata": {
        "id": "23DE0W12ZevG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "generator = pipeline('text-generation', model='Qwen/Qwen2.5-0.5B')\n",
        "\n",
        "# Define your prompt\n",
        "prompt = \"Write a short introduction about US capital city.\"\n",
        "\n",
        "# Generate text\n",
        "generated_text = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text[0]['generated_text'])\n",
        "\n",
        "free_gpu(generator.model)"
      ],
      "metadata": {
        "id": "DN1LFxdxTGVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break it up the generator function to examine step by step."
      ],
      "metadata": {
        "id": "kGoTHw8sbOJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "first_token_generated = False\n",
        "\n",
        "# (1) Specify the model and load tokenizer and model\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# (2) Define the input prompt - a text about communication history\n",
        "prompt = \"\"\"The history of human communication is a story of innovation. From ancient cave paintings and spoken language to the invention of writing systems, humans have constantly developed new methods to express ideas and share knowledge. The printing press revolutionized the spread of information, enabling books to be produced and distributed at an unprecedented scale. Centuries later, the invention of the telegraph, radio, and television further transformed how we connect with one another. But perhaps no advancement has reshaped communication more profoundly than the internet.\n",
        "Today, digital platforms allow billions of people to share messages, media, and experiences in real time. Social media, messaging apps, and video conferencing have broken down geographical barriers and created new ways of building communities. At the same time, these technologies raise important questions about privacy, information overload, and the nature of human interaction.\n",
        "Looking ahead, emerging technologies such as virtual reality, brain-computer interfaces, and artificial intelligence promise to once again redefine how we communicate. As we reflect on this history and anticipate the future, one question arises:\n",
        "\n",
        "How might the next wave of communication tools shape our relationships, societies, and sense of identity?\"\"\"\n",
        "\n",
        "# (3) Convert (Tokenize) prompt to the input format that model understands\n",
        "max_new_tokens = 100\n",
        "# tokenize the input prompt for the first output token\n",
        "# PS: prompt is the initial input sequence for LLM generation\n",
        "idx = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "start_time = total_time = time.time()\n",
        "times = []\n",
        "\n",
        "# (4) Main generation loop - generate tokens one by one\n",
        "for _ in range(max_new_tokens):\n",
        "\n",
        "    # (A) Set the current context for generation\n",
        "    idx_cond = idx\n",
        "    with torch.no_grad():\n",
        "        # (B) Generate predictions (token candidates) for next token\n",
        "        outputs = model(idx_cond)\n",
        "        # Get the logits (raw prediction scores) for each token predictions\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # (C) Select next token from the predictions generated in step (B)\n",
        "    logits = logits[:, -1, :] #  Select only the logits for the last token\n",
        "    probas = torch.softmax(logits, dim=-1) # Convert logits to probabilities using softmax\n",
        "    # Sample the next token from the probability distribution of the predicted tokens from step (B)\n",
        "    idx_next = torch.multinomial(probas, num_samples=1)\n",
        "    print(\"Next Token is:\", tokenizer.decode(idx_next[0], skip_special_tokens=True))\n",
        "    time_cost = time.time() - start_time\n",
        "    times.append(time_cost)\n",
        "\n",
        "    # Track time spent in token generation\n",
        "    if not first_token_generated:\n",
        "        print(f\"Time taken for generating the first token: {time_cost:.4f} seconds\")\n",
        "        first_token_generated = True\n",
        "    else:\n",
        "        print(f\"Time taken for generating a token: {time_cost:.4f} seconds\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # (D) Append the new token to the input sequence\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    # (E) Check if end-of-sequence token was generated\n",
        "    if idx_next.item() == tokenizer.eos_token_id:\n",
        "        print(\"\\n[Generation completed - EOS token reached]\")\n",
        "        break\n",
        "\n",
        "# Decode the entire generated sequence\n",
        "generated_text = tokenizer.decode(idx[0], skip_special_tokens=True)\n",
        "print(f\"Total time take for next token: {time.time() - total_time:.4f} seconds\")\n",
        "print(generated_text)\n",
        "\n",
        "# Free GPU memory\n",
        "free_gpu(model=model)"
      ],
      "metadata": {
        "id": "pWHKjduK2OgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the generation time taken for each token"
      ],
      "metadata": {
        "id": "k6agUcNkUwh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# First chart: First bar red, others blue\n",
        "plt.figure(figsize=(12, 4))\n",
        "# plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(times)), times, color=['red'] + ['blue'] * (len(times) - 1))\n",
        "plt.xlabel(\"Token ID\")\n",
        "plt.ylabel(\"Time Spent in Token Generation\")\n",
        "plt.title(\"LLM Generation Times for each token\")\n",
        "\n",
        "# # Second chart: Exclude the first element\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.bar(range(len(times) - 1), times[1:])\n",
        "# plt.xlabel(\"Token ID\")\n",
        "# plt.ylabel(\"Time Spent in Token Generation\")\n",
        "# plt.title(\"Token Generation Times (Excluded the initial prompt tokens)\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "yloug2pqRsl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, use_cache=True) \\\n",
        "            .to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "prompt = \"\"\"The history of human communication is a story of innovation. From ancient cave paintings and spoken language to the invention of writing systems, humans have constantly developed new methods to express ideas and share knowledge. The printing press revolutionized the spread of information, enabling books to be produced and distributed at an unprecedented scale. Centuries later, the invention of the telegraph, radio, and television further transformed how we connect with one another. But perhaps no advancement has reshaped communication more profoundly than the internet.\n",
        "Today, digital platforms allow billions of people to share messages, media, and experiences in real time. Social media, messaging apps, and video conferencing have broken down geographical barriers and created new ways of building communities. At the same time, these technologies raise important questions about privacy, information overload, and the nature of human interaction.\n",
        "Looking ahead, emerging technologies such as virtual reality, brain-computer interfaces, and artificial intelligence promise to once again redefine how we communicate. As we reflect on this history and anticipate the future, one question arises:\n",
        "\n",
        "How might the next wave of communication tools shape our relationships, societies, and sense of identity?\"\"\"\n",
        "\n",
        "num_interations = 100\n",
        "times_with_cache = []\n",
        "\n",
        "first_token_generated = False\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "idx = input_ids\n",
        "start_time = total_time = time.time()\n",
        "\n",
        "# (1) Define Key/Value Cache for faster generation\n",
        "past_key_values = None\n",
        "\n",
        "for _ in range(num_interations):\n",
        "    print(\"input_ids size: \" + str(input_ids.size()))\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids,\n",
        "          past_key_values=past_key_values, # (2) Use KV-cache from previous iteration\n",
        "          use_cache=True, # (2) Enable KV caching\n",
        "          max_new_tokens = 100,\n",
        "          min_new_tokens= 100)\n",
        "\n",
        "\n",
        "        logits = outputs.logits\n",
        "        # (3) Update KV Cache\n",
        "        past_key_values = outputs.past_key_values\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    logits = logits[:, -1, :]\n",
        "    probas = torch.softmax(logits, dim=-1)\n",
        "    generated_token_id = torch.multinomial(probas, num_samples=1) # Sample instead of argmax\n",
        "\n",
        "    # (4) Update input_ids with only the new token (using KV-cache)\n",
        "    input_ids = generated_token_id  # Note: Not concatenating with previous tokens due to KV-cache\n",
        "\n",
        "    print(\"Next token:\", tokenizer.decode(generated_token_id[0], skip_special_tokens=True))\n",
        "    idx = torch.cat((idx, generated_token_id), dim=1)\n",
        "\n",
        "    time_cost= time.time() - start_time\n",
        "    times_with_cache.append(time_cost)\n",
        "    if not first_token_generated:\n",
        "        print(f\"Time take for first token: {time_cost:.4f} seconds\")\n",
        "        first_token_generated = True\n",
        "    else:\n",
        "        print(f\"Time take for next token: {time_cost:.4f} seconds\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    if generated_token_id.item() == tokenizer.eos_token_id:\n",
        "        print(\"\\n[Generation completed - EOS token reached]\")\n",
        "        break\n",
        "\n",
        "generated_text = tokenizer.decode(idx[0], skip_special_tokens=True)\n",
        "print(f\"Total time take for next token: {time.time() - total_time:.4f} seconds\")\n",
        "print(generated_text)\n",
        "\n",
        "free_gpu(model=model)\n"
      ],
      "metadata": {
        "id": "vF1zCQIV736d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show results"
      ],
      "metadata": {
        "id": "kxqh6--jsQhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Display the time cost of LLM token generation without KV Cache\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(times)), times, color=['red'] + ['blue'] * (len(times) - 1))\n",
        "plt.xlabel(\"Token ID\")\n",
        "plt.ylabel(\"Time Spent in Token Generation\")\n",
        "plt.title(\"LLM Generation Times (No Cache) for each token\")\n",
        "\n",
        "\n",
        "# Display the time cost of LLM token generation with KV Cache\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(times_with_cache)), times_with_cache, color=['red'] + ['blue'] * (len(times_with_cache) - 1))\n",
        "plt.xlabel(\"Token ID\")\n",
        "plt.ylabel(\"Time Spent in Token Generation\")\n",
        "plt.title(\"LLM Generation Times (with Cache) for each token\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H5NNVW_5sS5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show Context"
      ],
      "metadata": {
        "id": "7GoNtrGnAE2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show how context impact generation"
      ],
      "metadata": {
        "id": "mJlA1eGVM49i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\\\n",
        "          .to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "prompt = \"\"\"The history of human communication is a story of innovation. From ancient cave paintings and spoken language to the invention of writing systems, humans have constantly developed new methods to express ideas and share knowledge. The printing press revolutionized the spread of information, enabling books to be produced and distributed at an unprecedented scale. Centuries later, the invention of the telegraph, radio, and television further transformed how we connect with one another. But perhaps no advancement has reshaped communication more profoundly than the internet.\n",
        "Today, digital platforms allow billions of people to share messages, media, and experiences in real time. Social media, messaging apps, and video conferencing have broken down geographical barriers and created new ways of building communities. At the same time, these technologies raise important questions about privacy, information overload, and the nature of human interaction.\n",
        "Looking ahead, emerging technologies such as virtual reality, brain-computer interfaces, and artificial intelligence promise to once again redefine how we communicate. As we reflect on this history and anticipate the future, one question arises:\n",
        "\n",
        "How might the next wave of communication tools shape our relationships, societies, and sense of identity?\"\"\"\n",
        "\n",
        "max_new_tokens = 50\n",
        "context_size = 2048 # Define the context size as needed for Qwen\n",
        "\n",
        "first_token_generated = False\n",
        "idx = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "start_time = total_time = time.time()\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    # Take the latest \"context_size\" tokens as model input\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    with torch.no_grad():\n",
        "        outputs = model(idx_cond)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # select next token from candidates based on probabilities.\n",
        "    logits = logits[:, -1, :]\n",
        "    probas = torch.softmax(logits, dim=-1)\n",
        "    idx_next = torch.multinomial(probas, num_samples=1)\n",
        "    print(\"Decode text generated:\", tokenizer.decode(idx_next[0], skip_special_tokens=True))\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    if idx_next.item() == tokenizer.eos_token_id:\n",
        "        print(\"\\n[Generation completed - EOS token reached]\")\n",
        "        break\n",
        "\n",
        "generated_text = tokenizer.decode(idx[0], skip_special_tokens=True)\n",
        "print(f\"Total time take for next token: {time.time() - total_time:.4f} seconds\")\n",
        "print(generated_text)\n",
        "\n",
        "# Free GPU memory\n",
        "free_gpu(model=model)"
      ],
      "metadata": {
        "id": "pHEQcsT_AECi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rfVwu9bd3_Gt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}