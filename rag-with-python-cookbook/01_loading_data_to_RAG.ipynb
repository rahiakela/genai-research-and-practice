{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/rag-with-python-cookbook/01_loading_data_to_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e3baf3",
      "metadata": {
        "id": "05e3baf3"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook uses the following Python packages:\n",
        "\n",
        "- `python-docx` (Word document reading)\n",
        "- `unstructured` (document partitioning)\n",
        "- `python-magic-bin` (file type detection)\n",
        "- `pandas` (data manipulation)\n",
        "- `PyPDF2` (PDF reading)\n",
        "- `pillow` (image processing)\n",
        "- `openpyxl` (Excel file reading)\n",
        "- `pdf2image` (PDF to image conversion)\n",
        "- `pytesseract` (OCR)\n",
        "- `openai` (OpenAI API)\n",
        "- `python-dotenv` (environment variable management)\n",
        "- `sqlalchemy` (database connection)\n",
        "- `psycopg2-binary` (PostgreSQL driver)\n",
        "- `moviepy` (video processing)\n",
        "- `pdfminer.six` (PDF text extraction)\n",
        "- `pi-heif` (HEIF image support)\n",
        "- `unstructured-inference` (document inference)\n",
        "\n",
        "Some helper functions may require additional dependencies. Install these packages using pip before running the notebook."
      ],
      "metadata": {
        "id": "VLdsQWz6XKow"
      },
      "id": "VLdsQWz6XKow"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f87a12ba",
      "metadata": {
        "id": "f87a12ba"
      },
      "outputs": [],
      "source": [
        "!pip install python-docx==1.1.2\n",
        "!pip install unstructured==0.17.2\n",
        "# !pip install python-magic-bin==0.4.14\n",
        "!pip install pandas==2.2.3\n",
        "!pip install PyPDF2==3.0.1\n",
        "!pip install pillow==11.2.1\n",
        "!pip install openpyxl==3.1.5\n",
        "# !pip install pdf2image==1.17.0\n",
        "# !pip install pytesseract==0.3.13\n",
        "# !pip install openai==1.82.1\n",
        "# !pip install python-dotenv==1.1.0\n",
        "# !pip install sqlalchemy==2.0.41\n",
        "# !pip install psycopg2-binary==2.9.10\n",
        "# !pip install moviepy==2.2.1\n",
        "# !pip install pdfminer.six==20250506\n",
        "# !pip install pi-heif==0.22.0\n",
        "# !pip install unstructured-inference==1.0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "528931c5",
      "metadata": {
        "id": "528931c5"
      },
      "source": [
        "## 1.1 Loading Word Files in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "add2c2f2",
      "metadata": {
        "id": "add2c2f2"
      },
      "source": [
        "**Option 1**: load word files using the python_docx library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/polzerdo55862/RAG-with-Python-Cookbook/raw/main/datasets/word_files/2023_Jan_7_Feature_Engineering_Techniques.docx"
      ],
      "metadata": {
        "id": "kL-uPfASXqxx"
      },
      "id": "kL-uPfASXqxx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "97da2421",
      "metadata": {
        "id": "97da2421"
      },
      "outputs": [],
      "source": [
        "# tag::python_docx[]\n",
        "import os\n",
        "from docx import Document\n",
        "\n",
        "file_path = \"2023_Jan_7_Feature_Engineering_Techniques.docx\"\n",
        "\n",
        "doc = Document(file_path)\n",
        "\n",
        "text = []\n",
        "for paragraph in doc.paragraphs:\n",
        "    text.append(paragraph.text)\n",
        "\n",
        "full_text = \"\\n\".join(text)\n",
        "# end::python_docx[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b0e1c8e7",
      "metadata": {
        "id": "b0e1c8e7",
        "outputId": "babdf46c-a145-416e-cc77-517a7e0c1df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n7 of the Most Used Feature Engineering Techniques\\nHands-on Feature Engineering with Scikit-Learn, Tensorflow, Pandas and Scipy\\n7 of the most used Feature Engineering Techniques\\u200a—\\u200aImage by the author\\n\\nTable of content\\nIntroduction\\n1. Encoding\\n 1.1 Label Encoding using Scikit-learn\\n 1.2 One-Hot Encoding using Scikit-learn, Pandas and Tensorflow\\n2. Feature Hashing\\n 2.1 Feature Hashing using Scikit-learn\\n3. Binning / Bucketizing\\n 3.1 Bucketizing using Pandas\\n 3.2 Bucketizing using Tensorflow\\n 3.3 Bucketizing using Scikit-learn\\n4. Transformer\\n 4.1 Log-Transformer using Numpy\\n 4.2 Box-Cox Function using Scipy\\n5. Normalize / Standardize\\n 5.1 Normalize and Standardize using Scikit-learn\\n6. Feature Crossing\\n 6.1 Feature Crossing in Polynomial Regression\\n 6.2 Feature Crossing and the Kernel-Trick\\n7. Principal Component Analysis (PCA)\\n 7.1 PCA using Scikit-learn\\nSummary\\nReferences\\n\\nIntroduction\\nFeature engineering describes the process of formulating relevant features that describe the underlying data science problem as accurately as possible and make it possible for algorithms to understand and learn patterns. In other words:\\nFeatures you provide serve as a way to communicate your own understanding and knowledge about the world to your model\\nEach feature describes a kind of information “piece”. The sum of these pieces allows the algorithm to draw conclusions about the target variables\\u200a—\\u200aat least if you have a data set that actually contains information about your target variable.\\nAccording to the Forbes magazine, Data Scientists spend about 80% of their time collecting and preparing relevant data, with the data cleaning and data organizing alone taking up about 60% of the time.\\nBut this time is well spent.\\nI believe that the quality of the data, as well as the proper preparation of the data set features, have a greater impact on the success of a machine learning model than any other part of the ML pipeline:\\nA standard Machine Learning pipeline\\u200a—\\u200aInspired by [Sarkar et al., 2018]\\nWhat Forbes magazine considers as “cleaning and organizing” is usually broken down into two to three subcategories in the ML pipeline (I have highlighted them with a yellow background in the image above):\\n(1) Data (Pre-)Processing: The initial preparation of the data\\u200a—\\u200afor example, smoothing a signal, dealing with outliers, etc.\\n(2) Feature engineering: Defining input features for our model\\u200a—\\u200ae.g., by converting an (acoustic) signal to the frequency domain using the fast Fourier transform (FFT) allows us to extract crucial information from the raw signal.\\n(3) Feature Selection: Selection of features that have a significant impact on the target variable. By selecting the important features and thus reducing the dimensionality, we can significantly reduce the modeling costs and increase the robustness and performance of the model.\\nWhy do we need Feature Engineering?\\nAndrew Ng frequently advocates a so-called data-centric approach, which emphasizes the importance of selecting and curating data, rather than simply trying to collect more and more data. The goal is to ensure that the data is of high quality and relevance to the problem being addressed and to continually improve the data set through data cleaning, feature engineering and data augmentation.\\nWhy do we need feature engineering and a data-centric approach when we have Deep Learning?\\nThis approach is particularly useful for use cases where it is costly or otherwise difficult to collect large amounts of data. [Brown, 2022] This might be the case when the data is difficult to access, or when there are strict regulations or other barriers to collecting and storing large amounts of data, e.g.:\\nIn the manufacturing sector, equipping production facilities with comprehensive sensor technology and connecting them to databases is expensive. As a result, many plants do not yet collect data at all. Even when machines are able to collect and store data, they are rarely connected to a central data lake.\\nIn the supply chain context, every company has only a certain number of orders that it processes every day. So if we want to predict the demand for a product that is in very irregular demand, we sometimes have only a few data points available.\\nIn such areas, feature engineering probably has the greatest leverage to increase the performance of the models. Here, the creativity of the engineers and data scientists is required to enhance the data set quality to a sufficient level. The process is rarely straightforward, but rather experimental and iterative.\\nWhen humans analyze data, they often use their past knowledge and experience to help them understand patterns and make predictions. For example, if someone is trying to estimate the temperature in different countries, they might consider the location of the country relative to the equator, since they know that temperatures tend to be higher near the equator.\\nHowever, a machine learning model does not have the same inherent understanding of concepts and relationships as a human does. It can only learn from the data it is given. Therefore, any background information or context that a human would use to solve a problem must be explicitly included in the data set in a numerical form.\\nAverage yearly temperature per country [Wikimedia]\\nSo what data do we need to make the model smarter than a human?\\nWe could use the Google Maps API to find the location coordinates (longitude and latitude) of each country. Additionally, we can gather information about the altitude of the region and the distance from the nearest body of water for each country. By collecting this extra data, we hope to identify and consider possible factors that could affect the temperature in each country.\\nLet’s say we have collected some data that might have an impact on the temperature, what next?\\nOnce we have enough data that describes the characteristics of the problem, we still have to make sure that the computer can understand the data. Categorical data, dates, etc. must be converted into numerical values.\\nIn this article, I am describing a few commonly used techniques for preparing raw data. Some techniques are used to convert categorical data into numerical values that can be understood by the machine learning model, such as encoding and vectorizing. Other techniques are used to address the distribution of the data, such as transformers and binning, which can help to normalize or standardize the data in some way. Still, other techniques are used to reduce the dimensionality of the dataset by generating new features, such as hashing and principal component analysis (PCA).\\nTry it yourself…\\nIf you want to follow the article and try the methods yourself, you can use the repo below, which contains the code snippets in a Jupyter Notebook and the data sets used:\\nGitHub - polzerdo55862/7-feature-engineering-techniques\\nYou can\\'t perform that action at this time. You signed in with another tab or window. You signed out in another tab or…github.com\\n\\n1. Encoding\\nFeature encoding is a process used to transform categorical data into numerical values that can be understood by ML algorithms. There are several types of encoding, including label encoding and one-hot encoding.\\nLabel and Hot Encoding\\u200a—\\u200aImage by the author\\nLabel encoding involves assigning a numeric value to each categorical value. This can be effective if there is an inherent order to the categorical values, such as grades from A to F, which can be encoded as numeric values from 1 to 5 (or 6). However, if there is no inherent order to the categorical values, label encoding may not be the best approach.\\nAlternatively, you can use One-hot encoding to transform categorical values into numerical values. In one-hot encoding, the column of categorical values is split into several new columns, one for each unique categorical value.\\nFor example, if the categorical values are grades from A to F:\\nwe would have five new columns, one for each grade.\\nEach row in the dataset would have a value of 1 in the column corresponding to its grade and 0 in all the other columns.\\nThis results in a so-called sparse matrix, where most of the values are 0.\\nThe disadvantage of one-hot encoding is that it can significantly increase the size of the dataset, which can be a problem if the column you want to encode contains hundreds or thousands of unique categorical values.\\nIn the following I am using Pandas, Scikit-learn or Tensorflow to apply label and one-hot encoding to your data set. For the example below, we are using the Census Income data set:\\nData set: Census Income [License: CC0: Public Domain]\\nhttps://archive.ics.uci.edu/ml/datasets/census+income\\nhttps://www.kaggle.com/datasets/uciml/adult-census-income\\nThe Census Income Data set describes the income of individuals in the United States. It includes their age, sex, marital status and other demographic information as well as their annual income, which is divided into two categories: over $50,000 or under $50,000.\\nFor the labelling example, we are using the “education” column of the census data set, which describes the highest level of education achieved by individuals within the population. It contains information such as whether an individual has completed high school, completed college, earned a graduate degree, or some other form of education.\\nFirst, let’s load the data set:\\nimport pandas as pd\\n\\n# load dataset - census income\\ncensus_income = pd.read_csv(r\\'../input/income/train.csv\\')\\n1.1 Label Encoding using Scikit-learn\\nLabel encoding is the simplest way to convert categorical values into numerical values. It is a simple process of assigning a numerical value to each category.\\nLabel Encoding— Image by the author\\nYou can find suitable libraries in Pandas, Scikit-Learn and Tensorflow. I am using the Scikit-Learns Label Encoder function. It is randomly assigning integers to the unique categorical values, which is the simplest way of encoding:\\nfrom sklearn import preprocessing\\n\\n# define and fit LabelEncoder\\nle = preprocessing.LabelEncoder()\\nle.fit(census_income[\"education\"])\\n\\n# Use the trained LabelEncoder to label the education column\\ncensus_income[\"education_labeled\"] = le.transform(census_income[\"education\"])\\n\\ndisplay(census_income[[\"education\", \"education_labeled\"]])\\n\\nThis way of encoding can cause problems for some algorithms because the assigned integers do not necessarily reflect any inherent order or relationship between the categories. For example, in the case described above, the algorithm may assume that the categories Doctorate (10) and HS-grad (11) are more similar to each other than categories Doctorate (10) and Bachelor (9) and that HS-grad (11) is “higher” than Doctorate (10).\\nLabel Encoding result interpretation\\u200a—\\u200aImage by the author\\nIf we have some knowledge about the specific domain or subject matter of a data set, we can use it to ensure that the label encoding process reflects any inherent order. For our example, we could try to label the education levels considering the order in which different degrees are obtained, e.g.:\\nDoctorate is a higher academic degree compared to a Master\\'s and Bachelor\\'s. The Master\\'s is higher than the Bachelor\\'s.\\n\\nTo apply this manual mapping to the data set, we can use the pandas.map function:\\neducation_labels = {\\'Doctorate\\':5, \\'Masters\\':4, \\'Bachelors\\':3, \\'HS-grad\\':2, \\'12th\\':1, \\'11th\\':0}\\n\\ncensus_income[\\'education_labeled_pandas\\']=census_income[\\'education\\'].map(education_labels)\\n\\ncensus_income[[\"education\", \"education_labeled_pandas\"]]\\n\\nBut how does this affect the model-building process?\\nLet\\'s build a simple linear regression model with the labeled encoded data:\\nFor the figure below, the target variable is defined as the probability that an individual has an income of more than $50,000.\\nIn the left figure, the categorical values (such as “Bachelor” and “Doctorate”) have been assigned randomly to numerical values.\\nIn the right figure, the numerical values assigned to the categorical values reflect the order in which the degrees are typically obtained, with higher education degrees being assigned higher numerical values.\\n→ The right figure shows a clear correlation between education level and income, which could be represented by a simple linear model.\\n→ In contrast, the left figure shows a relationship between the education attribute and the target variable, which would require a more complex model to accurately represent. This could affect the interpretability of the results, increase the risk of overfitting and increase the computational effort required to fit the model.\\nLabel Encoding: Linear Regression Model trained on ordered and unordered “education” feature\\u200a—\\u200aImage by the author\\nWhat do we do when values have no inherent order or we don’t have enough information to map it?\\nIf the categorical values do not have an inherent order at all, it may be better to use a method of encoding that converts the categories into a set of numeric variables without introducing any bias. One-hot encoding is one suitable method.\\n1.2 One-Hot Encoding using Scikit-learn, Pandas and Tensorflow\\nOne-hot encoding is a technique for converting categorical data into numerical data. It does this by creating a new binary column for each unique category in the data set and assigning a value of 1 to rows that belong to that category and a value of 0 to rows that do not.\\n→ This process helps to avoid introducing bias into the data by not assuming any inherent order between the categories.\\nOne-Hot Encoding\\u200a—\\u200aImage by the author\\nThe process of One-Hot Encoding is pretty straightforward. We could simply implement it by ourselves or use one of the existing functions. Scikit-learn has the .preprocessing.OneHotEncoder() function, Tensorflow the .one_hot() function, and Pandas the .get_dummies() function.\\nPandas.get_dummies()\\nimport pandas as pd\\n\\neducation_one_hot_pandas = pd.get_dummies(census_income[\"education\"], prefix=\\'education\\')\\neducation_one_hot_pandas.head(2)\\nSklearn.preprocessing.LabelBinarizer()\\nfrom sklearn import preprocessing\\n\\nlb = preprocessing.LabelBinarizer()\\nlb.fit(census_income[\"education\"])\\n\\neducation_one_hot_sklearn_binar = pd.DataFrame(lb.transform(census_income[\"education\"]), columns=lb.classes_)\\neducation_one_hot_sklearn_binar.head(2)\\nSklearn.preprocessing.OneHotEncoder()\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# define and fit the OneHotEncoder\\nohe = OneHotEncoder()\\nohe.fit(census_income[[\\'education\\']])\\n\\n# transform the data\\neducation_one_hot_sklearn = pd.DataFrame(ohe.transform(census_income[[\"education\"]]).toarray(), columns=ohe.categories_[0])\\neducation_one_hot_sklearn.head(3)\\nThe problem with one-hot encoding is that it can lead to large and sparse datasets with high dimensionality.\\nUsing one-hot encoding to convert a categorical feature with 10,000 unique values into numerical data would result in the creation of 10,000 new columns in the data set, each representing a different category. This can be a problem when working with large data sets, as it can quickly consume a lot of memory and computational resources.\\nIf memory and computer power are limited, it may be necessary to reduce the number of features in the data set to avoid running into memory or performance issues.\\nHow can we reduce dimensionality to save memory?\\nWhen doing this, it is important to try to minimize the loss of information as much as possible. This can be achieved by carefully selecting which features to retain or remove, by using techniques such as feature selection or dimensionality reduction to identify and remove redundant or irrelevant features. [Sklearn.org][Wikipedia, 2022]\\nIn this article I am describing two possible ways how to reduce the dimensionality of your data set:\\nFeature Hashing\\u200a—\\u200asee section 2. Feature Hashing\\nPrincipal Component Analysis (PCA)\\u200a—\\u200asee section 7. PCA\\nLoss of information vs. speed vs. memory\\nThere is probably not one “perfect” solution for reducing the number of dimensions in your data set. One method may be faster but may result in the loss of a lot of information, while the other method preserves more information but requires a lot of computing resources (which may also lead to memory issues).\\n\\n2. Feature Hashing\\nFeature hashing is primarily a dimensionality reduction technique and is often used in Natural Language Processing. However, hashing can also be useful when we want to vectorize categorical features with several hundred and thousand unique categories. With hashing, we can limit the increase of dimensionality by assigning several unique values to the same hash value.\\n→ Hashing is thus a low-memory alternative to OneHotEncoding and other feature vectorizing methods.\\nHashing works by applying a hash function to the features and using the hash values directly as indices, rather than building a hash table and looking up indices in it individually. The implementation in Sklearn is based on Weinberger [Weinberger et al., 2009].\\nWe can break it down into 2 steps:\\nStep 1: the categorical values are first converted to a hash value using a hash function. The implementation in Scikit-learn uses the 32-bit variant of MurmurHash3 for this.\\nHashing Trick\\u200a—\\u200aImage by the author\\n\\n\\nStep 2: Next, we reduce the dimensionality by applying a mod function to the feature values. Using the mod function, we calculate the remainder after dividing the hash value by n_features (the number of features of the output vector).\\nIt is advisable to use a power of two as the n_features parameter; otherwise, the features will not map evenly to the columns.\\nExample: Census Income data set →Encoding the Education column\\nThe “Education” column of the census income dataset contains 16 unique values:\\nWith one-hot encoding, 16 columns will be added to the dataset, each representing one of the 16 unique values.\\n→ To reduce the dimensions, we set n_features to 8.\\nThis inevitably leads to “collisions”, i.e. that different categorical values are mapped to the same hash columns. So in other words, we are assigning different values to the same bucket. Similar to what we are doing in Binning/Bucketizing (see section 3. Binning/Bucketizing). In feature hashing, we deal with collisions by simply chaining values that are assigned to the same bucket and storing them in a list (known as “separate chaining”).\\nFeature Hashing: Reducing the dimensionality by calculating the remainder as a new column index\\u200a—\\u200aImage by the author\\n\\n\\nA function that is doing the just described steps for us is the HashingVectorizer function from Scikit-learn.\\n2.1 Feature Hashing using Scikit-learn\\n\\nsklearn.feature_extraction.text.HashingVectorizer\\n\\nfrom sklearn.feature_extraction.text import HashingVectorizer\\n\\n\\n\\n3. Binning / Bucketizing\\nBinning is used for both categorical and numerical data. As the name suggests, the goal is to map the values of the features to “bins” and replace the original value with the value that represents the bin.\\nFor example, if we had a dataset with values ranging from 0 to 100 and we wanted to group those values into bins of size 10, we might create bins for values 0–9, 10–19, 20–29 and so on.\\n→ In this case, the original values would be replaced with the value that represents the bin to which they belong, such as 10, 20, 30, etc. This can help visualize and analyze the data.\\nSince we are reducing the number of unique values in the data set, it can help to:\\nprevent overfitting\\nincrease the robustness of the model and mitigate the influence of anomalies\\nreduce the model complexity and the required resources to train the model\\nSystematic binning can help the algorithm to detect underlying patterns more easily and efficiently. It is especially helpful if we can already form a hypothesis before we are defining the bins.\\nBinning can be used for both numeric and categorical values, e.g.:\\nBucketizing for categorical and numerical features\\u200a—\\u200aImage by the author\\nIn the following, I describe how this might look for numeric and categorical attributes using three examples:\\nNumeric\\u200a—\\u200aHow binning can be used when building a streaming recommender— a use case I found in the book Feature Engineering for Machine Learning by [Zheng, Alice, and Amanda Casari. 2018]\\nNumeric\\u200a—\\u200aCensus Income Data set: Binning applied to the “Age” column\\nCategorical —Binning in Supply Chain: Assign countries to bins, depending on the target variable\\n\\nExample 1: Streaming Recommender System\\u200a—\\u200aHow popular is a song or video?\\nIf you want to develop a recommender system, it is important to assign numerical values to the relative popularity of songs. One of the most significant attributes is the number of clicks, which is how often a user has listened to a song.\\nHowever, it is not necessarily true that a user who listens to a song 1000 times likes it 20 times as much as someone who has heard it 50 times. Binning can help prevent overfitting. Therefore, it can be beneficial to divide the number of clicks of songs into categories.: [Zheng, Alice and Amanda Casari. 2018]\\nClick count >=10: Very popular\\nClick count >=2: popular\\nClick count <2: neutral, no statement possible\\n\\nExample 2: Age-Income Relation\\nFor the second example, I am again using the census income data set. The goal is to group individuals into age buckets.\\nHypothesis\\u200a—\\u200aThe idea is that the specific age of a person may not have a significant impact on their income, but rather the stage of life they are in. For example, a person who is 20 and still in school may have a different income compared to a 30-year-old young professional. Similarly, a person who is still working full-time at age 60 may have a different income compared to someone who has retired at age 70, while there is probably not much difference in income if the person is 40 or 50.\\n3.1 Bucketizing using Pandas\\nTo bucketize the data by age, I am defining three “buckets”:\\nyoung\\u200a—\\u200a28 and younger\\nmiddle-aged\\u200a—\\u200a29 to 59\\nold-aged\\u200a—\\u200a60 and older\\n\\n\\n\\n3.2 Bucketizing using Tensorflow\\nTensorflow provides a module called feature columns that contains a range of functions designed to help with the pre-processing of raw data. Feature Columns are functions that organize and interpret raw data so that a machine learning algorithm can interpret it and use it to learn. [Google Developers, 2017]\\nTensorFlow’s feature_column.bucketized_column API provides a way to bucketize numeric data. This API takes in a numeric column and creates a bucketized column based on the specified boundaries. The input numeric column can be either a continuous or discrete value.\\n\\n\\nWe can do the same by using the KBinsDiscretizer from Scikit-learn.\\n\\n3.3 Bucketizing using Scikit-learn\\n\\n\\n\\n\\nOne last example for categorical values …\\nThere is not one universally best way to represent categorical variables numerically. The appropriate method to use, depends on the specific use case and the information that you want to convey to the model. Different methods of encoding can highlight different aspects of the data and it is important to choose the method that best suits the needs of your particular use case.\\n\\nExample 3: Bucketizing categorical values\\u200a—\\u200ae.g. Countries\\nSupply Chain\\u200a—\\u200aWhen assessing the reliability of a supplier/subcontractor, things like region, climate and type of transport can influence the accuracy of the delivery time. In our resource planning system, we usually store the supplier’s country and city. Since some countries are similar in some aspects and not at all in others, it may make sense to highlight aspects that are relevant to the use case.\\nIf we want to highlight the distance between countries and group countries in the same region, we could define bins for continents\\nIf we are more interested in pricing or possible revenues, the GDP per Capita is probably more important → Here we could group “high-cost” countries like USA, Germany and Japan in one bin\\nIf we want to highlight general weather conditions, we could put countries in the north like Canada and Norway in the same bin\\nExample: Bucketizing of countries using different attributes\\u200a—\\u200aImage by the author\\n\\n4. Transformer\\nTransformation techniques are methods used to change the form or distribution of a data set. Some common transformation techniques, such as the log or Box-Cox functions, are used to convert data that is not normally distributed into a form that is more symmetrical and follows a bell curve shape. These techniques can be useful when the data needs to meet certain assumptions that are required by certain statistical models or techniques. The specific transformation method that is used may depend on the desired outcome and the characteristics of the data.\\nTransformers: Transform the value distribution of your feature\\u200a—\\u200aImage by the author\\n4.1 Log-Transformer using Numpy\\nLog transformers are used to change the scale of the data by applying a logarithmic function to each value. This transformation is often used to convert highly skewed data into data that more closely resembles a normal distribution.\\nSkewed data is by no means something unusual, there are various situations where data is naturally or artificially skewed, e.g.:\\nThe frequency with which words are used in a language follows a pattern known as Zipf’s law\\nHow humans perceive different stimuli follows a pattern described by Stevens’ power function.\\nThe distribution of the data is not symmetrical and may have a long tail of values that are much larger or smaller than the majority of the data.\\nNormal vs. Power Law Distribution\\u200a—\\u200aImage by the author (Inspired by [Geeksforgeeks, 2022])\\nCertain types of algorithms may struggle to handle this type of data and may produce less accurate or reliable results. Applying a power transformation, such as the Box-Cox or log transformation, can help to adjust the scale of the data and make it more suitable for these algorithms to work with. [Geeksforgeeks, 2020]\\nLet’s have a look at how the transformers affect real-world data. Therefore I am using the World Population, Online News Popularity and Boston Housing data sets:\\nData Set 1: World Population [License: CC BY 3.0 IGO]\\nhttps://population.un.org/wpp/Download/Standard/CSV/\\nThe dataset contains the population of every country in the world. [United Nations, 2022]\\nData Set 2: Online News Popularity [License: CC0: Public Domain]\\nhttps://archive.ics.uci.edu/ml/datasets/online+news+popularity\\nThis dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks (popularity).\\nData set 3: Boston Housing Dataset [License: CC0: Public Domain]\\nhttps://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\\nThe Boston Housing data set was collected by the U.S. Census Bureau in the late 1970s and early 1980s. It contains information on the median values of homes in the Boston, Massachusetts area. The set includes 13 attributes, such as the average number of rooms per house, average distance to employment centers and property tax rate, as well as the median value of homes in the area.\\n\\nThe following function helps to plot the distributions before and after transforming the data sets:\\nIn the following, I am not only testing how the data looks like, after the transformation, I am also interested in how this could affect the model-building process. I am using Polynomial Regression to build simple regression models and compare the performance of the models. The input data for the models is just 2-dimensional:\\nfor the Only News Popularity data set we try to build a model which is predicting the “shares” based on the “n_tokens” of the online article\\u200a—\\u200aso we try to build a model which predicts the popularity of an article based on the number of tokens/length of the article\\nfor the World Population data set, we build a simple model which is predicting the population based on only one input feature, the area size of the country\\nExample 1: World Population data set\\u200a—\\u200aArea size of countries\\nplot_transformer(chosen_dataset = \"World Population\", chosen_transformation = \"log\")\\n\\nIn the graph, you can see that the original distribution, which was skewed to the left, has been transformed into a distribution that is more symmetrical.\\nLet\\'s try the same for the Online news popularity data set:\\nExample 2: Online news popularity data set\\u200a—\\u200aarticle popularity based on article length\\nplot_transformer(chosen_dataset = \"Online News Popularity\", chosen_transformation = \"log\")\\n\\nThis example shows the effect even better. The skewed data is transformed into almost “perfect” normally distributed data.\\nBut can this have a positive impact on the model-building process? For this purpose, I create polynomial models for the raw data and the transformed data and compare their performance:\\nI am aware this simple 2-dimensional example isn’t a representative example, as it is not possible to accurately predict the target variable using only one attribute. However, let’s see if the transformation is changing anything at all.\\nLet\\'s try it first with the Online News Popularity data set:\\n\\nPerformance of a Polynomial Regression Model trained on raw and transformed data\\u200a—\\u200aImage by the author\\nThe model trained on the transformed data is doing slightly better. Even though the difference in Absolute Error from 3213 to 3202 is not particularly large, it does indicate that transforming the data can have a positive effect on the training process.\\nBy plotting the transformed data and building the model, we can see, that the data is shifted to the right. This gives the model a little more “room” to fit the data:\\nWe use the just defined function to plot the polynomial model that showed the best performance:\\n\\nPolynomial Regression Model: Shares of online news— Image by the author\\nLet\\'s try the same with the World Population Data set:\\nPerformance of a Polynomial Regression Model trained on raw and transformed data\\u200a—\\u200aImage by the author\\nWe can see that the model performance is quite different between the raw and transformed data sets. While the models based on the raw data perform significantly better at low polynomial degrees, the models trained on the transformed data perform better when we build models with a higher polynomial degree.\\nPolynomial Regression Model: Country Population\\u200a—\\u200aImage by the author\\nJust for comparison, this is the linear model, that showed the best performance for the raw data:\\nPolynomial Regression Model: Country Population\\u200a—\\u200aImage by the author\\n\\n4.2 Box-Cox Function using Scipy\\nAnother very popular transformer function is the Box-Cox function which belongs to the group of power transformers.\\nPower transformers are a family of parametric transformations that aim to transform any distribution of data into a data set that is normally distributed and minimize variance and skewness. [Sklearn.org]\\nTo achieve this flexible transformation, the Box-Cox function is defined as follows:\\n\\nTo use the Box-Cox function, we must determine the transformation parameter lambda. If you do not manually specify a lambda, the transformer tries to find the best lambda by maximizing the likelihood function. [Rdocumentation.org] A suitable implementation can be found with the function scipy.stats.boxcox.\\nNOTE: The Box-Cox function must be used on data that is greater than zero.\\nBox-Cox transformation is more flexible than a log transformation because it can produce a variety of transformation shapes, including linear, quadratic and exponential, which may better fit the data. Additionally, Box-Cox transformations can be used to transform data that is both positively and negatively skewed, whereas a log transformation can only be used on positively skewed data.\\nWhat is the lambda parameter used for?\\nThe parameter lambda (λ) can be adjusted to tailor the transformation to the characteristics of the data being analyzed. A value of 0 for lambda produces a log transformation, while values between 0 and 1 create increasingly “strong” transformations. If lambda is set to 1, the function does not perform any transformation on the data.\\nTo show that the box-cox function can convert data that is not only skewed to the right, I am also using the AGE (“proportion of owner-occupied units built before 1940”) attribute from the Boston Housing data set.\\n\\n\\n\\n5. Normalize / Standardize\\nNormalizing and Standardizing are important preprocessing steps in Machine Learning. They can help algorithms to converge faster and can even increase the model accuracy.\\n5.1 Normalize and Standardize using Scikit-learn\\nScikit-learn’s MinMaxScaler scales features to a given range. It transforms features by scaling each feature to a given range between 0 and 1\\nScikit-learn’s StandardScaler transforms data to have a mean of 0 and a standard deviation of 1\\n\\n\\n6. Feature Crossing\\nFeature crossing is the process of connecting multiple features from a dataset to create a new feature. This can include combining data from other sources to emphasize existing correlations.\\nThere are no limits to creativity. First of all, it makes sense to make known correlations apparent by combining the attributes correctly, e.g.:\\nExample 1: Predicting the price of an apartment\\nLet\\'s say we have a series of apartments we want to sell and have the technical blueprints and thus the dimensions of the apartments available.\\nTo determine a reasonable price for the apartments, the specific dimensions are probably not as important as the total area: whether a room is 6 m * 7 m or 5.25 m * 8 m is not as important as the fact that the room has 42 m². If I only have the dimensions a and b, it makes sense to add the area as a feature as well.\\nPredicting the price of an apartment\\u200a—\\u200aImage by the author\\n\\nExample 2: Use technical know-how —Predicting the energy consumption of milling machines\\nA few years back, I was working on a regression model that would allow me to predict the energy consumption of a milling machine.\\nWhen implementing a solution for a specific domain, it is always worthwhile to look at the available literature. Since people were interested in being able to calculate the power requirements of a cutting machine for minimum 50 years, there are 50 years of research we can use.\\nEven if the existing formulas are only suitable for a rough calculation, we can use the know-how about identified correlations between attributes and the target variable, the energy consumption. In the figure below you can see some known relationships between the power demand and the variables.\\n→ We can make it easier for our model by highlighting these known relationships as features. We could for example cross the width of cut b and the cutting depth h (to calculate the cross-sectional area A) and define it as a new feature. This could help the training process, especially if we are using less complex models.\\nCalculation of the power requirement of a milling machine\\u200a—\\u200aImage by the author\\nBut we don’t just use it to prepare our dataset, some algorithms use Feature Crossing as a fundamental part of how they work.\\nSome ML methods and algorithms, already use feature crossing by default\\nTwo well-known ML techniques that use feature crossing are polynomial regression and the kernel trick (e.g., in combination with support vector regression).\\n6.1 Feature Crossing in Polynomial Regression\\nScikit-learn does not contain a function for polynomial regression, we create a pipeline out of:\\na feature transformation step and\\na linear regression model-building step.\\nBy combining and exponentiating features we generate several new features, which makes it possible to represent also non-linear relationships between the output variables.\\nLet’s say we want to build a regression model with a 2-dimensional input matrix X and the target variable y. Unless specifically defined, the feature transformation function (sklearn.PolynomialFeatures) transforms the matrix as follows:\\n\\nYou can see that the new matrix contains four new columns. The attributes are not only potentiated but also mutually crossed:\\nimport numpy as np\\nfrom sklearn.preprocessing import PolynomialFeatures\\n\\n# Raw input features\\nX = np.arange(6).reshape(3, 2)\\nprint(\"Raw input matrix:\")\\ndisplay(X)\\n\\n# Crossed features\\npoly = PolynomialFeatures(2)\\nprint(\"Transformed feature matrix:\")\\npoly.fit_transform(X)\\n\\nThis way we can model any imaginable relationship, we just have to choose the right polynomial degree.\\nThe following example shows a polynomial regression model (polynomial degree=5) which is predicting the median value of owner-occupied homes in Boston based on the attribute LSTAT (percentage of the population that has lower socio-economic status).\\n\\n\\n\\n6.2 Feature Crossing and the Kernel-Trick\\nAnother example where feature crossing is already inherently used is the kernel trick, for example in Support Vector Regression. Through a kernel function, we transform the data set into a higher dimensional space, which makes it possible to easily separate classes from each other.\\nFor the following example, I am generating a 2-dimensional data set, which is pretty hard to separate in a 2-dimensional space, at least for linear models.\\n\\nBy using the kernel trick, we generate a 3rd dimension. For this example, the kernel function is defined as:\\n\\nKernel-trick: A 2-dimensional dataset transferred to a 3-dimensional space\\u200a—\\u200aImage by the author\\nThe three-dimensional space allows the data to be separated using a simple linear classifier.\\n\\n7. Principal Component Analysis (PCA)\\nPrincipal Component Analysis, or PCA, reduces the dimension of a dataset by creating a set of new features derived from the raw features. Thus, similar to hashing, we reduce the complexity of the dataset and thus the computational effort required.\\nIn addition, it can help us visualize the data set. Data sets with multiple dimensions can be visualized in a lower-dimensional representation, while still preserving as much of the original variation as possible.\\nI’ll leave out a more in-depth explanation of how it works in this section, as there are already some excellent sources on the subject, e.g.:\\nJosh Starmer: StatQuest: Principal Component Analysis (PCA), Step-by-Step\\nSebastian Raschka: Principal Component Analysis in 3 simple steps\\nAs a quick summary, here are the 4 most important steps:\\nStandardize the data: Subtract the mean from each feature and scale the features to unit variance.\\nCalculate the covariance matrix of the standardized data: This matrix will contain the pairwise covariances between all of the features.\\nCompute the eigenvectors and eigenvalues of the covariance matrix: The eigenvectors determine the directions of the new feature space and represent the principal components and the eigenvalues determine their magnitude.\\nSelect the principal components: Select the first few principal components (usually the ones with the highest eigenvalues) and use them to project the data onto a lower-dimensional space. You can choose the number of components to keep, based on the amount of variance you want to preserve or the number of dimensions you want to reduce the data to.\\nHow PCA works\\u200a—\\u200aImage by the author (inspired by [Zheng, Alice, and Amanda Casari, 2018])\\n\\nI’ll show you how you can use PCA to create new features using the iris data set.\\nData Set: Iris Data Set [License: CC0: Public Domain]\\nhttps://archive.ics.uci.edu/ml/datasets/iris\\nhttps://www.kaggle.com/datasets/uciml/iris\\nThe Iris dataset is a dataset containing information about three species of Iris flowers (Iris setosa, Iris virginica and Iris versicolor). It includes data on the length and width of the sepals and petals of the flowers in centimeters and contains 150 observations.\\n\\nSo first, let\\'s load the data set and have a look at the distribution of the data in the four dimensions (Sepal Length, Sepal Width, Petal Length, Petal Width)\\n\\nYou can already see from the distribution that the individual classes can be distinguished relatively well based on the PetalWidth and PetalLength. In the other two dimensions, the distributions overlap strongly.\\nThis suggests that the PetalWidth and PetalLength are probably more important for our model than the other two dimensions. So if I were only allowed to use 2 dimensions as features for my model, I would take these two. If we plot these two dimensions, we get the following figure:\\n\\nNot too bad, we see that by choosing the “most important” attributes we can reduce the dimension and lose as little information as possible.\\nPCA goes one step further, it centers and scales the data and projects the data onto newly generated dimensions. This way we are not bound to the existing dimensions. Similar to a 3D representation of a dataset that we rotate in space until we find an orientation that allows us to separate the classes as easily as possible:\\nIris Data set visualized in a 3d plot\\u200a—\\u200aImage by the Author\\nOverall, then, PCA transforms, scales and rotates the data until a new set of dimensions (the principal components) is found that capture the most important information of the original data.\\nA suitable function to apply PCA to your data set can be found in Scikit-learn, sklearn.decomposition.PCA.\\n7.1 PCA using Scikit-learn\\nIf you compare the plot of the first two principal components (PC1 and PC2) with the two-dimensional plot of PetalWidth and PetalLength, you can see that the most important information in the data set is preserved. In addition, the data is centered and scaled.\\nPCA1 and PCA2 of the transformed Iris data\\u200a—\\u200aImage by the author\\n\\nSummary\\nThe techniques described in the article can be applied to any type of data, but they can not replace methods that are specific to the field you are working in.\\nA good example is the field of acoustics. Let’s assume you have no idea about acoustics and signal processing ….\\nHow would you process the airborne sound signal and what features would you extract from it?\\nWould you think of decomposing the signal into individual spectral components to understand the composition of the signal? Probably not. Fortunately, someone before you has asked the same question and defined some suitable transformation methods, such as the Fast Fourier Transform (FFT).\\nStanding on the shoulders of giants\\nEven though the technology is advanced, that doesn’t mean we can’t use insights that were made decades ago. We can learn from these past experiences and use them to make our model-building process more efficient. So it’s always a good idea to spend some time to understand the field your data is from if you want to create models that are effective at solving real-world problems.\\nContinue reading…\\nIf you enjoyed reading and want to continue reading about Machine Learning concepts, algorithms and applications, you can find a list of my related articles here:\\nMachine Learning: Concepts, Techniques and Applications\\n dmnkplzr.medium.com\\nIf you are interested in signing up with Medium to get unlimited access to all stories, you can support me by using my referral link. This will earn me a small commission at no additional cost to you.\\nThanks for reading!\\n\\nReferences\\nBox, G E P, and D R Cox. “An Analysis of Transformations.” : 43.\\nBrown, Sara. 2022. “Why It’s Time for ‘Data-Centric Artificial Intelligence.’” MIT Sloan. https://mitsloan.mit.edu/ideas-made-to-matter/why-its-time-data-centric-artificial-intelligence (October 30, 2022).\\neducative.io. “Feature Selection and Feature Engineering\\u200a—\\u200aMachine Learning System Design.” Educative: Interactive Courses for Software Developers. https://www.educative.io/courses/machine-learning-system-design/q2AwDN4nZ73 (November 25, 2022).\\nFeature Engineering with H2O\\u200a—\\u200aDmitry Larko, Senior Data Scientist, H2O.Ai. 2017. https://www.youtube.com/watch?v=irkV4sYExX4 (September 5, 2022).\\nfeatureranking.com. “Case Study: Predicting Income Status.” [www.featureranking.com.](http://www.featureranking.com./) https://www.featureranking.com/tutorials/machine-learning-tutorials/case-study-predicting-income-status/ (November 26, 2022).\\nGeeksforgeeks. 2020. “Python | Box-Cox Transformation.” GeeksforGeeks. https://www.geeksforgeeks.org/box-cox-transformation-using-python/ (December 25, 2022).\\nGoogle Developers. 2017. “Intro to Feature Engineering with TensorFlow\\u200a—\\u200aMachine Learning Recipes #9.” https://www.youtube.com/watch?v=d12ra3b_M-0 (September 5, 2022).\\nGoogle Developers. “Introducing TensorFlow Feature Columns.” https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html (September 21, 2022).\\nHeavy.ai. 2022. “What Is Feature Engineering? Definition and FAQs | HEAVY.AI.” https://www.heavy.ai/technical-glossary/feature-engineering (September 19, 2022).\\nheavy.ai. “Feature Engineering.” https://www.heavy.ai/technical-glossary/feature-engineering.\\nKoehrsten, Will. “Introduction to Manual Feature Engineering.” https://kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering (September 5, 2022).\\nKousar, Summer. “EDA on Cyber Security Salary.” https://kaggle.com/code/summerakousar/eda-on-cyber-security-salary (September 7, 2022).\\nMoody, John. 1988. “Fast Learning in Multi-Resolution Hierarchies.” In Advances in Neural Information Processing Systems, Morgan-Kaufmann. https://proceedings.neurips.cc/paper/1988/hash/82161242827b703e6acf9c726942a1e4-Abstract.html (November 28, 2022).\\nPoon, Wing. 2022. “Feature Engineering for Machine Learning (1/3).” Medium. https://towardsdatascience.com/feature-engineering-for-machine-learning-a80d3cdfede6 (September 19, 2022).\\nPoon, Wing. “Feature Engineering for Machine Learning (2/3) Part 2: Feature Generation.” : 10.\\npydata.org. “Pandas.Get_dummies\\u200a—\\u200aPandas 1.5.2 Documentation.” https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html (November 25, 2022).\\nRaschka, Sebastian. “Principal Component Analysis.” Sebastian Raschka, PhD. https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html (December 17, 2022).\\nRdocumentation.org. “Boxcox Function\\u200a—\\u200aRDocumentation.” https://www.rdocumentation.org/packages/EnvStats/versions/2.7.0/topics/boxcox (December 25, 2022).\\nSarkar, Dipanjan. 2019a. “Categorical Data.” Medium. https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63 (September 19, 2022).\\nSarkar, Dipanjan. 2019b. “Continuous Numeric Data.” Medium. https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b (September 19, 2022).\\nSarkar, Dipanjan, Raghav Bali, and Tushar Sharma. 2018. Practical Machine Learning with Python. Berkeley, CA: Apress. http://link.springer.com/10.1007/978-1-4842-3207-1 (November 25, 2022).\\nSiddhartha. 2020. “Demonstration of TensorFlow Feature Columns (Tf.Feature_column).” ML Book. https://medium.com/ml-book/demonstration-of-tensorflow-feature-columns-tf-feature-column-3bfcca4ca5c4 (September 21, 2022).\\nSklearn.org. “6.2. Feature Extraction.” scikit-learn. https://scikit-learn/stable/modules/feature_extraction.html (November 28, 2022a).\\nspark.apache.org. “Extracting, Transforming and Selecting Features\\u200a—\\u200aSpark 3.3.1 Documentation.” https://spark.apache.org/docs/latest/ml-features (December 1, 2022).\\nTensorflow.org. “Tf.One_hot | TensorFlow v2.11.0.” TensorFlow. https://www.tensorflow.org/api_docs/python/tf/one_hot (November 25, 2022).\\nUnited Nations, Department of Economic and Social Affairs, Population Division (2022). World Population Prospects 2022, Online Edition.\\nValohai.com. 2022. “What Is a Machine Learning Pipeline?” https://valohai.com/machine-learning-pipeline/ (September 19, 2022).\\nVotava, Adam. 2022. “Keeping Up With Data #105.” Medium. https://adamvotava.medium.com/keeping-up-with-data-105-6a2a8a41f4b6 (October 21, 2022).\\nWeinberger, Kilian et al. 2009. “Feature Hashing for Large Scale Multitask Learning.” In Proceedings of the 26th Annual International Conference on Machine Learning\\u200a—\\u200aICML ’09, Montreal, Quebec, Canada: ACM Press, 1–8. http://portal.acm.org/citation.cfm?doid=1553374.1553516 (December 25, 2022).\\nWhat Makes a Good Feature?\\u200a—\\u200aMachine Learning Recipes #3. 2016. https://www.youtube.com/watch?v=N9fDIAflCMY (September 5, 2022).\\nWikimedia. “Average yearly temperature per country.png.” https://commons.wikimedia.org/wiki/File:Average_yearly_temperature_per_country.png (December 25, 2022).\\nWikipedia. 2022. “Feature Hashing.” Wikipedia. https://en.wikipedia.org/w/index.php?title=Feature_hashing&oldid=1114513799 (November 28, 2022).\\nZheng, Alice and Amanda Casari. 2018. Feature Engineering for Machine Learning.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "full_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20a45a7",
      "metadata": {
        "id": "b20a45a7"
      },
      "source": [
        "**Option 2**: load word files using the unstructured library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b8bf7132",
      "metadata": {
        "id": "b8bf7132"
      },
      "outputs": [],
      "source": [
        "# tag::unstructured[]\n",
        "from unstructured.partition.docx import partition_docx\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "elements = partition_docx(filename=file_path)\n",
        "\n",
        "list_of_elements = []\n",
        "\n",
        "for element in elements:\n",
        "    element_dict = {\n",
        "        \"element_id\": element.id,\n",
        "        \"file_path\": file_path,\n",
        "        \"category\": element.category,  # e.g. \"Title\", \"NarrativeText\", \"ListItem\"\n",
        "        \"text\": element.text,\n",
        "        \"last_modified\": element.metadata.last_modified,\n",
        "    }\n",
        "\n",
        "    list_of_elements.append(element_dict)\n",
        "\n",
        "elements_df = pd.DataFrame(list_of_elements)\n",
        "# end::unstructured[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2292c546",
      "metadata": {
        "id": "2292c546",
        "outputId": "9dd70ddc-7c2f-4c54-d908-cffb82a80c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         element_id  \\\n",
              "0  135f726911a68beceb56d92e2b9d10bc   \n",
              "1  f275447183f11b993f2a87d4b428299b   \n",
              "2  9dacd0881e31b366756a6cc20884f661   \n",
              "3  3bec63fc43107e87aae98bbaf5313196   \n",
              "4  b1b29811f875047fef0bab817d6325c5   \n",
              "\n",
              "                                        file_path           category  \\\n",
              "0  2023_Jan_7_Feature_Engineering_Techniques.docx              Title   \n",
              "1  2023_Jan_7_Feature_Engineering_Techniques.docx              Title   \n",
              "2  2023_Jan_7_Feature_Engineering_Techniques.docx      NarrativeText   \n",
              "3  2023_Jan_7_Feature_Engineering_Techniques.docx              Title   \n",
              "4  2023_Jan_7_Feature_Engineering_Techniques.docx  UncategorizedText   \n",
              "\n",
              "                                                text        last_modified  \n",
              "0  7 of the Most Used Feature Engineering Techniques  2025-06-30T10:28:28  \n",
              "1  Hands-on Feature Engineering with Scikit-Learn...  2025-06-30T10:28:28  \n",
              "2  7 of the most used Feature Engineering Techniq...  2025-06-30T10:28:28  \n",
              "3                                   Table of content  2025-06-30T10:28:28  \n",
              "4                                       Introduction  2025-06-30T10:28:28  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-153a9f9a-33cf-49cb-b55f-323c63876e52\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>element_id</th>\n",
              "      <th>file_path</th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>last_modified</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135f726911a68beceb56d92e2b9d10bc</td>\n",
              "      <td>2023_Jan_7_Feature_Engineering_Techniques.docx</td>\n",
              "      <td>Title</td>\n",
              "      <td>7 of the Most Used Feature Engineering Techniques</td>\n",
              "      <td>2025-06-30T10:28:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>f275447183f11b993f2a87d4b428299b</td>\n",
              "      <td>2023_Jan_7_Feature_Engineering_Techniques.docx</td>\n",
              "      <td>Title</td>\n",
              "      <td>Hands-on Feature Engineering with Scikit-Learn...</td>\n",
              "      <td>2025-06-30T10:28:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9dacd0881e31b366756a6cc20884f661</td>\n",
              "      <td>2023_Jan_7_Feature_Engineering_Techniques.docx</td>\n",
              "      <td>NarrativeText</td>\n",
              "      <td>7 of the most used Feature Engineering Techniq...</td>\n",
              "      <td>2025-06-30T10:28:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3bec63fc43107e87aae98bbaf5313196</td>\n",
              "      <td>2023_Jan_7_Feature_Engineering_Techniques.docx</td>\n",
              "      <td>Title</td>\n",
              "      <td>Table of content</td>\n",
              "      <td>2025-06-30T10:28:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b1b29811f875047fef0bab817d6325c5</td>\n",
              "      <td>2023_Jan_7_Feature_Engineering_Techniques.docx</td>\n",
              "      <td>UncategorizedText</td>\n",
              "      <td>Introduction</td>\n",
              "      <td>2025-06-30T10:28:28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-153a9f9a-33cf-49cb-b55f-323c63876e52')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-153a9f9a-33cf-49cb-b55f-323c63876e52 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-153a9f9a-33cf-49cb-b55f-323c63876e52');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e88eec98-8d0c-4705-9061-2a6afa10289f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e88eec98-8d0c-4705-9061-2a6afa10289f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e88eec98-8d0c-4705-9061-2a6afa10289f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "elements_df",
              "summary": "{\n  \"name\": \"elements_df\",\n  \"rows\": 397,\n  \"fields\": [\n    {\n      \"column\": \"element_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 397,\n        \"samples\": [\n          \"6266c2ac53cd31187bdbff1b35ec37d3\",\n          \"ec95c657b6c3d68ab29860fb48ec9cb6\",\n          \"0fd64e96f7d9a2edb40e4a727e5f26ba\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_path\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2023_Jan_7_Feature_Engineering_Techniques.docx\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"NarrativeText\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 383,\n        \"samples\": [\n          \"Let's say we have a series of apartments we want to sell and have the technical blueprints and thus the dimensions of the apartments available.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_modified\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2025-06-30T10:28:28\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "elements_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42caeb4",
      "metadata": {
        "id": "b42caeb4"
      },
      "source": [
        "## 1.2 Loading PDF Files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/polzerdo55862/RAG-with-Python-Cookbook/raw/main/datasets/pdf_files/2023_Jan_7_Feature_Engineering_Techniques.pdf"
      ],
      "metadata": {
        "id": "SvtZG-p_bN92"
      },
      "id": "SvtZG-p_bN92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "708ad118",
      "metadata": {
        "id": "708ad118"
      },
      "outputs": [],
      "source": [
        "# tag::load_pdf_using_PyPDF2[]\n",
        "import PyPDF2\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"2023_Jan_7_Feature_Engineering_Techniques.pdf\"\n",
        "\n",
        "with open(file_path, \"rb\") as file:\n",
        "    reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "    # Initialize an empty string to store the extracted text\n",
        "    list_of_pages = []\n",
        "    page_counter = 1\n",
        "\n",
        "    for page in reader.pages:\n",
        "        page_dict = {\n",
        "            \"file_name\": reader.metadata.get(\"/Title\"),\n",
        "            \"producer\": reader.metadata.get(\"/Producer\"),\n",
        "            \"page_number\": page_counter,\n",
        "            \"text\": page.extract_text(),\n",
        "            \"images\": page.images,\n",
        "        }\n",
        "\n",
        "        list_of_pages.append(page_dict)\n",
        "\n",
        "        page_counter += 1\n",
        "# end::load_pdf_using_PyPDF2[]\n",
        "\n",
        "# Convert the list of pages to a pandas DataFrame\n",
        "pages_df = pd.DataFrame(list_of_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b449b9eb",
      "metadata": {
        "id": "b449b9eb",
        "outputId": "e19b7c4b-d2e1-4016-d47a-2d376b4d6ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   file_name  \\\n",
              "0  2023_Jan_7_Feature_Engineering_Techniques   \n",
              "1  2023_Jan_7_Feature_Engineering_Techniques   \n",
              "2  2023_Jan_7_Feature_Engineering_Techniques   \n",
              "3  2023_Jan_7_Feature_Engineering_Techniques   \n",
              "4  2023_Jan_7_Feature_Engineering_Techniques   \n",
              "\n",
              "                             producer  page_number  \\\n",
              "0  Skia/PDF m131 Google Docs Renderer            1   \n",
              "1  Skia/PDF m131 Google Docs Renderer            2   \n",
              "2  Skia/PDF m131 Google Docs Renderer            3   \n",
              "3  Skia/PDF m131 Google Docs Renderer            4   \n",
              "4  Skia/PDF m131 Google Docs Renderer            5   \n",
              "\n",
              "                                                text  \\\n",
              "0  7\\nof\\nthe\\nMost\\nUsed\\nFeature\\nEngineering\\n...   \n",
              "1  3.2\\nBucketizing\\nusing\\nTensorflow\\n3.3\\nBuck...   \n",
              "2  A\\nstandard\\nMachine\\nLearning\\npipeline — Ins...   \n",
              "3  ●\\nI\\nn\\nthe\\nsupply\\nchain\\ncontext\\n,\\nevery...   \n",
              "4  Once\\nwe\\nhave\\nenough\\ndata\\nthat\\ndescribes\\...   \n",
              "\n",
              "                                 images  \n",
              "0     [File(name=X7.png, data: 2.2 kB)]  \n",
              "1                                    []  \n",
              "2  [File(name=X17.png, data: 692 Byte)]  \n",
              "3    [File(name=X20.png, data: 2.6 kB)]  \n",
              "4    [File(name=X26.png, data: 1.5 kB)]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e2572d3-8850-4445-94f4-912df9cca4e8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>producer</th>\n",
              "      <th>page_number</th>\n",
              "      <th>text</th>\n",
              "      <th>images</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023_Jan_7_Feature_Engineering_Techniques</td>\n",
              "      <td>Skia/PDF m131 Google Docs Renderer</td>\n",
              "      <td>1</td>\n",
              "      <td>7\\nof\\nthe\\nMost\\nUsed\\nFeature\\nEngineering\\n...</td>\n",
              "      <td>[File(name=X7.png, data: 2.2 kB)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023_Jan_7_Feature_Engineering_Techniques</td>\n",
              "      <td>Skia/PDF m131 Google Docs Renderer</td>\n",
              "      <td>2</td>\n",
              "      <td>3.2\\nBucketizing\\nusing\\nTensorflow\\n3.3\\nBuck...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023_Jan_7_Feature_Engineering_Techniques</td>\n",
              "      <td>Skia/PDF m131 Google Docs Renderer</td>\n",
              "      <td>3</td>\n",
              "      <td>A\\nstandard\\nMachine\\nLearning\\npipeline — Ins...</td>\n",
              "      <td>[File(name=X17.png, data: 692 Byte)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023_Jan_7_Feature_Engineering_Techniques</td>\n",
              "      <td>Skia/PDF m131 Google Docs Renderer</td>\n",
              "      <td>4</td>\n",
              "      <td>●\\nI\\nn\\nthe\\nsupply\\nchain\\ncontext\\n,\\nevery...</td>\n",
              "      <td>[File(name=X20.png, data: 2.6 kB)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023_Jan_7_Feature_Engineering_Techniques</td>\n",
              "      <td>Skia/PDF m131 Google Docs Renderer</td>\n",
              "      <td>5</td>\n",
              "      <td>Once\\nwe\\nhave\\nenough\\ndata\\nthat\\ndescribes\\...</td>\n",
              "      <td>[File(name=X26.png, data: 1.5 kB)]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e2572d3-8850-4445-94f4-912df9cca4e8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e2572d3-8850-4445-94f4-912df9cca4e8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e2572d3-8850-4445-94f4-912df9cca4e8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6494a0e8-ec72-4db0-8393-8a8de549e83d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6494a0e8-ec72-4db0-8393-8a8de549e83d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6494a0e8-ec72-4db0-8393-8a8de549e83d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pages_df",
              "summary": "{\n  \"name\": \"pages_df\",\n  \"rows\": 56,\n  \"fields\": [\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2023_Jan_7_Feature_Engineering_Techniques\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"producer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Skia/PDF m131 Google Docs Renderer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 1,\n        \"max\": 56,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 56,\n        \"samples\": [\n          \"7\\nof\\nthe\\nMost\\nUsed\\nFeature\\nEngineering\\nTechniques\\nHands-on\\nFeature\\nEngineering\\nwith\\nScikit-Learn,\\nTensorflow,\\nPandas\\nand\\nScipy\\n7\\nof\\nthe\\nmost\\nused\\nFeature\\nEngineering\\nTechniques\\u200a\\u2014\\u200aImage\\nby\\nthe\\nauthor\\nTable\\nof\\ncontent\\nIntroduction\\n1.\\nEncoding\\n1.1\\nLabel\\nEncoding\\nusing\\nScikit-learn\\n1.2\\nOne-Hot\\nEncoding\\nusing\\nScikit-learn,\\nPandas\\nand\\nTensorflow\\n2.\\nFeature\\nHashing\\n2.1\\nFeature\\nHashing\\nusing\\nScikit-learn\\n3.\\nBinning\\n/\\nBucketizing\\n3.1\\nBucketizing\\nusing\\nPandas\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"images\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "pages_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b86d86",
      "metadata": {
        "id": "54b86d86"
      },
      "source": [
        "## 1.3 Loading and Handling CSV and Excel Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e77df492",
      "metadata": {
        "id": "e77df492"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/polzerdo55862/RAG-with-Python-Cookbook/raw/main/datasets/csv_files/census-income.xlsx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f9b4a6c0",
      "metadata": {
        "id": "f9b4a6c0"
      },
      "outputs": [],
      "source": [
        "###########################################################################################################\n",
        "# Define the file path to the Word document\n",
        "###########################################################################################################\n",
        "# tag::create_additional_table_column[]\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"census-income.xlsx\"\n",
        "df_excel = pd.read_excel(io=file_path)\n",
        "\n",
        "\n",
        "def create_text_description_of_row(row):\n",
        "    row[\"text_description\"] = (\n",
        "        f\"\"\"The candidate {row['age']} years old is working in the\n",
        "            {row['workclass']} sector. The candidate was born in\n",
        "            {row['native-country']}, is {row['marital-status']}\n",
        "            and has a {row['relationship']} relationship.\n",
        "            The candidate has a {row['education']} degree\n",
        "            and is working as a {row['occupation']}.\n",
        "            The income of the candidate is {row['income']}.\"\"\"\n",
        "    )\n",
        "\n",
        "    return row\n",
        "\n",
        "\n",
        "# Apply the function create_text_description_of_row to each row of the data frame\n",
        "df_extended = df_excel.apply(create_text_description_of_row, axis=1)\n",
        "# end::create_additional_table_column[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7cebebdb",
      "metadata": {
        "id": "7cebebdb",
        "outputId": "f059298b-3e51-447b-fbd4-d1fa3fd96c5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    The candidate 39 years old is working in the\\n...\n",
              "1    The candidate 50 years old is working in the\\n...\n",
              "2    The candidate 38 years old is working in the\\n...\n",
              "3    The candidate 53 years old is working in the\\n...\n",
              "4    The candidate 28 years old is working in the\\n...\n",
              "Name: text_description, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The candidate 39 years old is working in the\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The candidate 50 years old is working in the\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The candidate 38 years old is working in the\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The candidate 53 years old is working in the\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The candidate 28 years old is working in the\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Display the first 5 text_description of the dataset\n",
        "df_extended[\"text_description\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1296a62a",
      "metadata": {
        "id": "1296a62a",
        "outputId": "8f3f6d7f-2504-4462-cc80-4663cf83c57e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The candidate 39 years old is working in the\\n            State-gov sector. The candidate was born in\\n            United-States, is Never-married\\n            and has a Not-in-family relationship.\\n            The candidate has a Bachelors degree\\n            and is working as a Adm-clerical.\\n            The income of the candidate is <=50K.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df_extended[\"text_description\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72751749",
      "metadata": {
        "id": "72751749"
      },
      "source": [
        "## 1.4 Querying a PostgreSQL Database"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c9bbbe",
      "metadata": {
        "id": "98c9bbbe"
      },
      "source": [
        "```\n",
        "CREATE USER rag_user WITH PASSWORD 'raguserpassword123';\n",
        "GRANT ALL ON ALL TABLES IN SCHEMA public TO rag_user;\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c3c88c4",
      "metadata": {
        "id": "2c3c88c4"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "################################################################################\n",
        "# Querying the postgres database using SQLAlchemy\n",
        "################################################################################\n",
        "\n",
        "\n",
        "username = os.getenv(\"POSTGRESQL_USER\")  # Your PostgreSQL username\n",
        "password = os.getenv(\"POSTGRESQL_PASSWORD\")  # Your PostgreSQL password\n",
        "host = os.getenv(\"DB_HOST\", \"localhost\")  # Default to localhost if not provided\n",
        "port = os.getenv(\"DB_PORT\", \"5432\")  # Default to 5432 if not provided\n",
        "database = os.getenv(\"DB_NAME\", \"postgres\")  # Database name (e.g., postgres)\n",
        "\n",
        "# tag::query_postgres[]\n",
        "import os\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "connection_string = (\n",
        "    f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}\"\n",
        ")\n",
        "engine = create_engine(connection_string)\n",
        "\n",
        "with engine.connect() as connection:\n",
        "    query = \"\"\"SELECT * FROM categories ORDER BY category_id ASC \"\"\"\n",
        "    result = pd.read_sql(query, connection)\n",
        "    print(result)\n",
        "# end::query_postgres[]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bbc42f7",
      "metadata": {
        "id": "4bbc42f7"
      },
      "source": [
        "### 1.5 Loading Audio Files by Using Speech-to-Text Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd28b4f",
      "metadata": {
        "id": "bfd28b4f"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import openai\n",
        "\n",
        "# Set OPENAI_API_KEY environment variable using the value from the .env file\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# tag::transform_audio_to_text[]\n",
        "import os\n",
        "import openai\n",
        "\n",
        "audio_file_path = \"../datasets/audio_files/harvard.wav\"\n",
        "\n",
        "# initialize the OpenAI client with your API key\n",
        "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "with open(audio_file_path, \"rb\") as audio_file:\n",
        "    transcription = client.audio.transcriptions.create(\n",
        "        model=\"whisper-1\", file=audio_file\n",
        "    )\n",
        "# end::transform_audio_to_text[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52742c98",
      "metadata": {
        "id": "52742c98",
        "outputId": "84d91ae7-dd0e-4628-8f2f-0257effcb0e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transcription(text='The stale smell of old beer lingers. It takes heat to bring out the odor. A cold dip restores health and zest. A salt pickle tastes fine with ham. Tacos al pastor are my favorite. A zestful food is the hot cross bun.', logprobs=None)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transcription"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea003db8",
      "metadata": {
        "id": "ea003db8"
      },
      "source": [
        "### 1.6 Extracting Text from Images and PDFs Using OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c4b2df0",
      "metadata": {
        "id": "3c4b2df0",
        "outputId": "bf4c4073-0b47-4aed-eb1f-0e24b7a30bcb"
      },
      "outputs": [
        {
          "ename": "TesseractNotFoundError",
          "evalue": "tesseract is not installed or it's not in your PATH. See README file for more information.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\pytesseract\\pytesseract.py:275\u001b[39m, in \u001b[36mrun_tesseract\u001b[39m\u001b[34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     proc = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msubprocess_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1038\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1035\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1036\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1048\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1550\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     hp, ht, pid, tid = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[32m   1552\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1554\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1559\u001b[39m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[32m   1560\u001b[39m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1563\u001b[39m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[32m   1564\u001b[39m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] The system cannot find the file specified",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mTesseractNotFoundError\u001b[39m                    Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m image = Image.open(fp=\u001b[33m\"\u001b[39m\u001b[33m../datasets/images/example_finance_reporting_slide.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Use Tesseracst to do OCR on the image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m text = \u001b[43mpytesseract\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# end::extract_text_from_financial_reporting_slide_tesseract[]\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m###########################################################################################################\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Define the file path to the Word document\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m###########################################################################################################\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# tag::extract_text_from_images[]\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\pytesseract\\pytesseract.py:486\u001b[39m, in \u001b[36mimage_to_string\u001b[39m\u001b[34m(image, lang, config, nice, output_type, timeout)\u001b[39m\n\u001b[32m    481\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    484\u001b[39m args = [image, \u001b[33m'\u001b[39m\u001b[33mtxt\u001b[39m\u001b[33m'\u001b[39m, lang, config, nice, timeout]\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBYTES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDICT\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSTRING\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\pytesseract\\pytesseract.py:489\u001b[39m, in \u001b[36mimage_to_string.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    481\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    484\u001b[39m args = [image, \u001b[33m'\u001b[39m\u001b[33mtxt\u001b[39m\u001b[33m'\u001b[39m, lang, config, nice, timeout]\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    487\u001b[39m     Output.BYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(*(args + [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[32m    488\u001b[39m     Output.DICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: run_and_get_output(*args)},\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     Output.STRING: \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    490\u001b[39m }[output_type]()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\pytesseract\\pytesseract.py:352\u001b[39m, in \u001b[36mrun_and_get_output\u001b[39m\u001b[34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m save(image) \u001b[38;5;28;01mas\u001b[39;00m (temp_name, input_filename):\n\u001b[32m    342\u001b[39m     kwargs = {\n\u001b[32m    343\u001b[39m         \u001b[33m'\u001b[39m\u001b[33minput_filename\u001b[39m\u001b[33m'\u001b[39m: input_filename,\n\u001b[32m    344\u001b[39m         \u001b[33m'\u001b[39m\u001b[33moutput_filename_base\u001b[39m\u001b[33m'\u001b[39m: temp_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m: timeout,\n\u001b[32m    350\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     \u001b[43mrun_tesseract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_output(\n\u001b[32m    354\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[33m'\u001b[39m\u001b[33moutput_filename_base\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextsep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    355\u001b[39m         return_bytes,\n\u001b[32m    356\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\pytesseract\\pytesseract.py:280\u001b[39m, in \u001b[36mrun_tesseract\u001b[39m\u001b[34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[39m\n\u001b[32m    278\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractNotFoundError()\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m timeout_manager(proc, timeout) \u001b[38;5;28;01mas\u001b[39;00m error_string:\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m proc.returncode:\n",
            "\u001b[31mTesseractNotFoundError\u001b[39m: tesseract is not installed or it's not in your PATH. See README file for more information."
          ]
        }
      ],
      "source": [
        "# tag::extract_text_from_financial_reporting_slide_tesseract[]\n",
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "# Load the sample .png file\n",
        "image = Image.open(fp=\"../datasets/images/example_finance_reporting_slide.png\")\n",
        "\n",
        "# Use Tesseracst to do OCR on the image\n",
        "text = pytesseract.image_to_string(image)\n",
        "# end::extract_text_from_financial_reporting_slide_tesseract[]\n",
        "\n",
        "###########################################################################################################\n",
        "# Define the file path to the Word document\n",
        "###########################################################################################################\n",
        "# tag::extract_text_from_images[]\n",
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "file_path = \"../datasets/images/2023_Jan_7_Feature_Engineering_Techniques.pdf\"\n",
        "\n",
        "# Convert PDF to a list of images\n",
        "images = convert_from_path(pdf_path=file_path)\n",
        "\n",
        "text = []\n",
        "for i, image in enumerate(images):\n",
        "    page_text = pytesseract.image_to_string(image)\n",
        "    text.append(page_text)\n",
        "# end::extract_text_from_images[]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2986fa73",
      "metadata": {
        "id": "2986fa73"
      },
      "source": [
        "### 1.7 Extracting Text from Images using Multimodal Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71798de0",
      "metadata": {
        "id": "71798de0"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "# Set OPENAI_API_KEY environment variable using the value from the .env file\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# tag::extract_text_from_financial_reporting_slide[]\n",
        "import os\n",
        "from PIL import Image\n",
        "import base64\n",
        "import openai\n",
        "\n",
        "png_file_path = \"../datasets/images/example_finance_reporting_slide.png\"\n",
        "\n",
        "with open(png_file_path, \"rb\") as image_file:\n",
        "    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "    prompt = (\n",
        "        \"Extract the text from the image attached. Make sure to only \"\n",
        "        \"extract only the text. If there is no text in the image, \"\n",
        "        \"please return with the sentence 'No text found in the image.\"\n",
        "    )\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o\",  # define the model to use\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                        },\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "# end::extract_text_from_financial_reporting_slide[]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eff671a",
      "metadata": {
        "id": "5eff671a"
      },
      "source": [
        "### 1.8 Generating Text Summaries for Images Using Multimodal Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec5eee2",
      "metadata": {
        "id": "bec5eee2"
      },
      "outputs": [],
      "source": [
        "# Load the environment variables from the .env file\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# tag::generate_text_summaries_for_images[]\n",
        "import base64\n",
        "import openai\n",
        "\n",
        "image_path = \"../datasets/images/vietnam.png\"\n",
        "\n",
        "with open(image_path, \"rb\") as image_file:\n",
        "    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "    prompt = (\n",
        "        \"You are an assistant for visually impaired users. \"\n",
        "        \"Describe the image in detail.\"\n",
        "    )\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                        },\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "# end::generate_text_summaries_for_images[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f347f47b",
      "metadata": {
        "id": "f347f47b",
        "outputId": "2179f204-9b22-4bd2-a8ae-dd2a2a31cf32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The image depicts a city skyline at dusk, with the sky exhibiting shades of blue and purple. Prominent modern skyscrapers line the scene, with the tallest building having a distinctive sharp spire and bright lights at the top. Another nearby building has a unique curved top with a protruding structure. The buildings are a mix of glass and steel, reflecting the evening lights.\\n\\nIn front of the skyline, there is a calm body of water that mirror the lights and colors from the buildings. On the left side, a brightly lit dock or platform juts into the water, adding a touch of yellow from the artificial lights. The overall ambience is serene, with the city coming to life as daylight fades.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fde1cea8",
      "metadata": {
        "id": "fde1cea8"
      },
      "source": [
        "### 1.9 Generating Text Summaries for Embedded Tables Using Multimodal Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb60ee0b",
      "metadata": {
        "id": "eb60ee0b",
        "outputId": "c7436f9e-9694-49f5-82e4-fc4ed92f8c7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Failed to get OCRAgent instance: No module named 'unstructured_pytesseract'\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Could not get the OCRAgent instance. Please check the OCR package and the OCR_AGENT environment variable.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\utils\\ocr_models\\ocr_interface.py:47\u001b[39m, in \u001b[36mOCRAgent.get_instance\u001b[39m\u001b[34m(ocr_agent_module, language)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     loaded_class = \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\utils\\ocr_models\\tesseract_ocr.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_pytesseract\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlxml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m etree\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'unstructured_pytesseract'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m texts = []\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# partition the PDF file into its elements\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m raw_pdf_elements = \u001b[43mpartition_pdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhi_res\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m raw_pdf_elements:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33munstructured.documents.elements.Table\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(element)):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\documents\\elements.py:585\u001b[39m, in \u001b[36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n\u001b[32m    588\u001b[39m     unique_element_ids: \u001b[38;5;28mbool\u001b[39m = call_args.get(\u001b[33m\"\u001b[39m\u001b[33munique_element_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:816\u001b[39m, in \u001b[36madd_filetype.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[32m--> \u001b[39m\u001b[32m816\u001b[39m     elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements:\n\u001b[32m    819\u001b[39m         \u001b[38;5;66;03m# NOTE(robinson) - Attached files have already run through this logic\u001b[39;00m\n\u001b[32m    820\u001b[39m         \u001b[38;5;66;03m# in their own partitioning function\u001b[39;00m\n\u001b[32m    821\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m element.metadata.attached_to_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:774\u001b[39m, in \u001b[36madd_metadata.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m     call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n\u001b[32m    777\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m call_args.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata_filename\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\chunking\\dispatch.py:74\u001b[39m, in \u001b[36madd_chunking_strategy.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The decorated function is replaced with this one.\"\"\"\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[32m     77\u001b[39m call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf.py:228\u001b[39m, in \u001b[36mpartition_pdf\u001b[39m\u001b[34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, metadata_filename, metadata_last_modified, chunking_strategy, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, password, pdfminer_line_margin, pdfminer_char_margin, pdfminer_line_overlap, pdfminer_word_margin, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m exactly_one(filename=filename, file=file)\n\u001b[32m    227\u001b[39m languages = check_language_args(languages \u001b[38;5;129;01mor\u001b[39;00m [], ocr_languages)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf_or_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_line_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_line_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_char_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_char_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_line_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_line_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_word_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_word_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf.py:341\u001b[39m, in \u001b[36mpartition_pdf_or_image\u001b[39m\u001b[34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, languages, metadata_last_modified, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, password, pdfminer_line_margin, pdfminer_char_margin, pdfminer_line_overlap, pdfminer_word_margin, ocr_agent, table_ocr_agent, **kwargs)\u001b[39m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings():\n\u001b[32m    340\u001b[39m         warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m         elements = \u001b[43m_partition_pdf_or_image_local\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspooled_to_bytes_io_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m            \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlast_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpdf_text_extractable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_text_extractable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m            \u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpdfminer_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m         out_elements = _process_uncategorized_text_elements(elements)\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m strategy == PartitionStrategy.FAST:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\utils.py:216\u001b[39m, in \u001b[36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs):\n\u001b[32m    215\u001b[39m     run_check()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf.py:690\u001b[39m, in \u001b[36m_partition_pdf_or_image_local\u001b[39m\u001b[34m(filename, file, is_image, infer_table_structure, include_page_breaks, languages, ocr_languages, ocr_mode, model_name, hi_res_model_name, pdf_image_dpi, metadata_last_modified, pdf_text_extractable, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, analysis, analyzed_image_output_dir_path, starting_page_number, extract_forms, form_extraction_skip_tables, pdf_hi_res_max_pages, password, pdfminer_config, ocr_agent, table_ocr_agent, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m     \u001b[38;5;66;03m# NOTE(christine): merged_document_layout = extracted_layout + inferred_layout\u001b[39;00m\n\u001b[32m    684\u001b[39m     merged_document_layout = merge_inferred_with_extracted_layout(\n\u001b[32m    685\u001b[39m         inferred_document_layout=inferred_document_layout,\n\u001b[32m    686\u001b[39m         extracted_layout=extracted_layout,\n\u001b[32m    687\u001b[39m         hi_res_model_name=hi_res_model_name,\n\u001b[32m    688\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     final_document_layout = \u001b[43mprocess_file_with_ocr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmerged_document_layout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextracted_layout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextracted_layout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_layout_dumper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_layout_dumper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    705\u001b[39m     inferred_document_layout = process_data_with_model(\n\u001b[32m    706\u001b[39m         file,\n\u001b[32m    707\u001b[39m         is_image=is_image,\n\u001b[32m   (...)\u001b[39m\u001b[32m    710\u001b[39m         password=password,\n\u001b[32m    711\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\utils.py:216\u001b[39m, in \u001b[36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs):\n\u001b[32m    215\u001b[39m     run_check()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf_image\\ocr.py:201\u001b[39m, in \u001b[36mprocess_file_with_ocr\u001b[39m\u001b[34m(filename, out_layout, extracted_layout, is_image, infer_table_structure, ocr_agent, ocr_languages, ocr_mode, pdf_image_dpi, ocr_layout_dumper, password, table_ocr_agent)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(filename) \u001b[38;5;129;01mor\u001b[39;00m os.path.isfile(filename):\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFile \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m not found!\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf_image\\ocr.py:186\u001b[39m, in \u001b[36mprocess_file_with_ocr\u001b[39m\u001b[34m(filename, out_layout, extracted_layout, is_image, infer_table_structure, ocr_agent, ocr_languages, ocr_mode, pdf_image_dpi, ocr_layout_dumper, password, table_ocr_agent)\u001b[39m\n\u001b[32m    184\u001b[39m     extracted_regions = extracted_layout[i] \u001b[38;5;28;01mif\u001b[39;00m i < \u001b[38;5;28mlen\u001b[39m(extracted_layout) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PILImage.open(image_path) \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         merged_page_layout = \u001b[43msupplement_page_layout_with_ocr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpage_layout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextracted_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextracted_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_layout_dumper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_layout_dumper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m         merged_page_layouts.append(merged_page_layout)\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DocumentLayout.from_pages(merged_page_layouts)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\utils.py:216\u001b[39m, in \u001b[36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs):\n\u001b[32m    215\u001b[39m     run_check()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf_image\\ocr.py:229\u001b[39m, in \u001b[36msupplement_page_layout_with_ocr\u001b[39m\u001b[34m(page_layout, image, infer_table_structure, ocr_agent, ocr_languages, ocr_mode, extracted_regions, ocr_layout_dumper, table_ocr_agent)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ocr_agent == OCR_AGENT_PADDLE:\n\u001b[32m    228\u001b[39m     language = tesseract_to_paddle_language(ocr_languages)\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m _ocr_agent = \u001b[43mOCRAgent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocr_agent_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ocr_mode == OCRMode.FULL_PAGE.value:\n\u001b[32m    231\u001b[39m     ocr_layout = _ocr_agent.get_layout_from_image(image)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\utils\\ocr_models\\ocr_interface.py:52\u001b[39m, in \u001b[36mOCRAgent.get_instance\u001b[39m\u001b[34m(ocr_agent_module, language)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     51\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to get OCRAgent instance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not get the OCRAgent instance. Please check the OCR package and the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOCR_AGENT environment variable.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m     )\n",
            "\u001b[31mRuntimeError\u001b[39m: Could not get the OCRAgent instance. Please check the OCR package and the OCR_AGENT environment variable."
          ]
        }
      ],
      "source": [
        "# tag::extract_embedded_tables_from_pdf[]\n",
        "import os\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "pdf_file_path = \"../datasets/pdf_files/adult_data_article.pdf\"\n",
        "\n",
        "tables = []\n",
        "texts = []\n",
        "\n",
        "# partition the PDF file into its elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=pdf_file_path,\n",
        "    strategy=\"hi_res\",\n",
        ")\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        tables.append(str(element))\n",
        "\n",
        "# end::extract_embedded_tables_from_pdf[]\n",
        "\n",
        "# tag::summarize_tables[]\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def summarize_tables(row):\n",
        "    summary_prompt = f\"\"\"You are an assistant tasked with summarizing tables. \\\n",
        "                    Give a concise summary of the table. Table chunk: {row.table}\"\"\"\n",
        "\n",
        "    # Initialize the OpenAI API client and generate the table summary\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=150,\n",
        "    )\n",
        "\n",
        "    row[\"table_summary\"] = response.choices[0].message.content\n",
        "\n",
        "    return row\n",
        "\n",
        "\n",
        "# create a pandas dataframe from the tables\n",
        "tables_df = pd.DataFrame(tables, columns=[\"table\"])\n",
        "\n",
        "# add a column to the dataframe to store the summaries\n",
        "tables_df = tables_df.apply(summarize_tables, axis=1)\n",
        "# end::summarize_tables[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a31c4b8",
      "metadata": {
        "id": "7a31c4b8"
      },
      "outputs": [],
      "source": [
        "tables_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42af1f0",
      "metadata": {
        "id": "d42af1f0",
        "outputId": "9be29d14-9e3a-4cc8-ee26-c7afba6f351b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tables_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m answered_question\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# generate the answer to the user's question\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# as context we using the first entry in the tables_df\u001b[39;00m\n\u001b[32m     52\u001b[39m answered_question = build_prompt_and_generate_answer(\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     user_question=user_question, found_table=\u001b[43mtables_df\u001b[49m.iloc[\u001b[32m0\u001b[39m]\n\u001b[32m     54\u001b[39m )\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(answered_question)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# end::test_ask_a_question[]\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'tables_df' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# tag::test_ask_a_question[]\n",
        "# define a random question to the embedded table\n",
        "user_question = \"What are the education levels of the people working in Sales?\"\n",
        "\n",
        "\n",
        "def build_prompt_and_generate_answer(user_question, found_table):\n",
        "    \"\"\"\n",
        "    This function builds a prompt using the user's question and the context of the table\n",
        "    and generates an answer using the OpenAI API\n",
        "\n",
        "    Parameters:\n",
        "        user_question: the question asked by the user\n",
        "        found_table: the table context to generate the answer from\n",
        "\n",
        "    Returns:\n",
        "        answered_question: the answer to the user's question\n",
        "    \"\"\"\n",
        "\n",
        "    question_prompt = f\"\"\"You are an assistant using the content from PDFs \\\n",
        "                        to answer questions. Below you can find the \\\n",
        "                        user's question and relevant context. Please use the \\\n",
        "                        context to generate an answer to the user's question.\n",
        "\n",
        "                        # User question: {user_question}\n",
        "\n",
        "                        # Context:\n",
        "\n",
        "                        ## Table summary:\n",
        "                        {found_table.table_summary}\n",
        "\n",
        "                        ## Table content:\n",
        "                        {found_table.table}\"\"\".stripe()\n",
        "\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    answered_question = (\n",
        "        client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": question_prompt}],\n",
        "            temperature=0.7,\n",
        "            max_tokens=150,\n",
        "        )\n",
        "        .choices[0]\n",
        "        .message.content\n",
        "    )\n",
        "\n",
        "    return answered_question\n",
        "\n",
        "\n",
        "# generate the answer to the user's question\n",
        "# as context we using the first entry in the tables_df\n",
        "answered_question = build_prompt_and_generate_answer(\n",
        "    user_question=user_question, found_table=tables_df.iloc[0]\n",
        ")\n",
        "\n",
        "print(answered_question)\n",
        "# end::test_ask_a_question[]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ccec3c7",
      "metadata": {
        "id": "8ccec3c7"
      },
      "source": [
        "### 1.10 Parsing PDFs with Multiple Media Content Using Unstructured and Multimodal Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c76ad1",
      "metadata": {
        "id": "b1c76ad1",
        "outputId": "c883980c-3fb1-4220-be6c-28dee41af168"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to get OCRAgent instance: No module named 'unstructured_pytesseract'\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Could not get the OCRAgent instance. Please check the OCR package and the OCR_AGENT environment variable.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\utils\\ocr_models\\ocr_interface.py:47\u001b[39m, in \u001b[36mOCRAgent.get_instance\u001b[39m\u001b[34m(ocr_agent_module, language)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     loaded_class = \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\utils\\ocr_models\\tesseract_ocr.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_pytesseract\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlxml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m etree\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'unstructured_pytesseract'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m image_output_dir = \u001b[33m\"\u001b[39m\u001b[33m../datasets/extracted_content_from_pdfs/images\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# get elements using the function extract_pdf_elements\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m raw_pdf_elements = \u001b[43mpartition_pdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mImage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# categorize elements by type\u001b[39;00m\n\u001b[32m     21\u001b[39m tables = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\documents\\elements.py:585\u001b[39m, in \u001b[36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n\u001b[32m    588\u001b[39m     unique_element_ids: \u001b[38;5;28mbool\u001b[39m = call_args.get(\u001b[33m\"\u001b[39m\u001b[33munique_element_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:816\u001b[39m, in \u001b[36madd_filetype.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[32m--> \u001b[39m\u001b[32m816\u001b[39m     elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements:\n\u001b[32m    819\u001b[39m         \u001b[38;5;66;03m# NOTE(robinson) - Attached files have already run through this logic\u001b[39;00m\n\u001b[32m    820\u001b[39m         \u001b[38;5;66;03m# in their own partitioning function\u001b[39;00m\n\u001b[32m    821\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m element.metadata.attached_to_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:774\u001b[39m, in \u001b[36madd_metadata.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m     call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n\u001b[32m    777\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m call_args.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata_filename\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\chunking\\dispatch.py:74\u001b[39m, in \u001b[36madd_chunking_strategy.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The decorated function is replaced with this one.\"\"\"\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m elements = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[32m     77\u001b[39m call_args = get_call_args_applying_defaults(func, *args, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf.py:228\u001b[39m, in \u001b[36mpartition_pdf\u001b[39m\u001b[34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, metadata_filename, metadata_last_modified, chunking_strategy, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, password, pdfminer_line_margin, pdfminer_char_margin, pdfminer_line_overlap, pdfminer_word_margin, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m exactly_one(filename=filename, file=file)\n\u001b[32m    227\u001b[39m languages = check_language_args(languages \u001b[38;5;129;01mor\u001b[39;00m [], ocr_languages)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf_or_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_line_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_line_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_char_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_char_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_line_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_line_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdfminer_word_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_word_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf.py:341\u001b[39m, in \u001b[36mpartition_pdf_or_image\u001b[39m\u001b[34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, languages, metadata_last_modified, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, password, pdfminer_line_margin, pdfminer_char_margin, pdfminer_line_overlap, pdfminer_word_margin, ocr_agent, table_ocr_agent, **kwargs)\u001b[39m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings():\n\u001b[32m    340\u001b[39m         warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m         elements = \u001b[43m_partition_pdf_or_image_local\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspooled_to_bytes_io_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m            \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlast_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpdf_text_extractable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_text_extractable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m            \u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpdfminer_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfminer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m         out_elements = _process_uncategorized_text_elements(elements)\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m strategy == PartitionStrategy.FAST:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\utils.py:216\u001b[39m, in \u001b[36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs):\n\u001b[32m    215\u001b[39m     run_check()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf.py:690\u001b[39m, in \u001b[36m_partition_pdf_or_image_local\u001b[39m\u001b[34m(filename, file, is_image, infer_table_structure, include_page_breaks, languages, ocr_languages, ocr_mode, model_name, hi_res_model_name, pdf_image_dpi, metadata_last_modified, pdf_text_extractable, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, analysis, analyzed_image_output_dir_path, starting_page_number, extract_forms, form_extraction_skip_tables, pdf_hi_res_max_pages, password, pdfminer_config, ocr_agent, table_ocr_agent, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m     \u001b[38;5;66;03m# NOTE(christine): merged_document_layout = extracted_layout + inferred_layout\u001b[39;00m\n\u001b[32m    684\u001b[39m     merged_document_layout = merge_inferred_with_extracted_layout(\n\u001b[32m    685\u001b[39m         inferred_document_layout=inferred_document_layout,\n\u001b[32m    686\u001b[39m         extracted_layout=extracted_layout,\n\u001b[32m    687\u001b[39m         hi_res_model_name=hi_res_model_name,\n\u001b[32m    688\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     final_document_layout = \u001b[43mprocess_file_with_ocr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmerged_document_layout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextracted_layout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextracted_layout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_layout_dumper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_layout_dumper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    705\u001b[39m     inferred_document_layout = process_data_with_model(\n\u001b[32m    706\u001b[39m         file,\n\u001b[32m    707\u001b[39m         is_image=is_image,\n\u001b[32m   (...)\u001b[39m\u001b[32m    710\u001b[39m         password=password,\n\u001b[32m    711\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\utils.py:216\u001b[39m, in \u001b[36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs):\n\u001b[32m    215\u001b[39m     run_check()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf_image\\ocr.py:201\u001b[39m, in \u001b[36mprocess_file_with_ocr\u001b[39m\u001b[34m(filename, out_layout, extracted_layout, is_image, infer_table_structure, ocr_agent, ocr_languages, ocr_mode, pdf_image_dpi, ocr_layout_dumper, password, table_ocr_agent)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(filename) \u001b[38;5;129;01mor\u001b[39;00m os.path.isfile(filename):\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFile \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m not found!\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf_image\\ocr.py:186\u001b[39m, in \u001b[36mprocess_file_with_ocr\u001b[39m\u001b[34m(filename, out_layout, extracted_layout, is_image, infer_table_structure, ocr_agent, ocr_languages, ocr_mode, pdf_image_dpi, ocr_layout_dumper, password, table_ocr_agent)\u001b[39m\n\u001b[32m    184\u001b[39m     extracted_regions = extracted_layout[i] \u001b[38;5;28;01mif\u001b[39;00m i < \u001b[38;5;28mlen\u001b[39m(extracted_layout) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PILImage.open(image_path) \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         merged_page_layout = \u001b[43msupplement_page_layout_with_ocr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpage_layout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextracted_regions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextracted_regions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mocr_layout_dumper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_layout_dumper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_ocr_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m         merged_page_layouts.append(merged_page_layout)\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DocumentLayout.from_pages(merged_page_layouts)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\utils.py:216\u001b[39m, in \u001b[36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs):\n\u001b[32m    215\u001b[39m     run_check()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\pdf_image\\ocr.py:229\u001b[39m, in \u001b[36msupplement_page_layout_with_ocr\u001b[39m\u001b[34m(page_layout, image, infer_table_structure, ocr_agent, ocr_languages, ocr_mode, extracted_regions, ocr_layout_dumper, table_ocr_agent)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ocr_agent == OCR_AGENT_PADDLE:\n\u001b[32m    228\u001b[39m     language = tesseract_to_paddle_language(ocr_languages)\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m _ocr_agent = \u001b[43mOCRAgent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocr_agent_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocr_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ocr_mode == OCRMode.FULL_PAGE.value:\n\u001b[32m    231\u001b[39m     ocr_layout = _ocr_agent.get_layout_from_image(image)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\unstructured\\partition\\utils\\ocr_models\\ocr_interface.py:52\u001b[39m, in \u001b[36mOCRAgent.get_instance\u001b[39m\u001b[34m(ocr_agent_module, language)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     51\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to get OCRAgent instance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not get the OCRAgent instance. Please check the OCR package and the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOCR_AGENT environment variable.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m     )\n",
            "\u001b[31mRuntimeError\u001b[39m: Could not get the OCRAgent instance. Please check the OCR package and the OCR_AGENT environment variable."
          ]
        }
      ],
      "source": [
        "# tag::extract_pdf_elements[]\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "import os\n",
        "\n",
        "# set the OCR agent to tesseract\n",
        "os.environ[\"OCR_AGENT\"] = \"tesseract\"\n",
        "\n",
        "pdf_file_path = \"../datasets/pdf_files/adult_data_article.pdf\"\n",
        "image_output_dir = \"../datasets/extracted_content_from_pdfs/images\"\n",
        "\n",
        "# get elements using the function extract_pdf_elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=pdf_file_path,\n",
        "    extract_images_in_pdf=True,\n",
        "    extract_image_block_types=[\"Image\", \"Table\"],\n",
        "    extract_image_block_to_payload=False,\n",
        "    extract_image_block_output_dir=image_output_dir,\n",
        ")\n",
        "\n",
        "# categorize elements by type\n",
        "tables = []\n",
        "texts = []\n",
        "titles = []\n",
        "\n",
        "# fill the just created lists with the elements\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        tables.append(str(element))\n",
        "    elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
        "        texts.append(str(element))\n",
        "    elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
        "        titles.append(str(element))\n",
        "# end::extract_pdf_elements[]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c28585d",
      "metadata": {
        "id": "9c28585d"
      },
      "source": [
        "### 1.11 Loading Videos Using Speech-to-Text and Multimodal Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1e7db7a",
      "metadata": {
        "id": "b1e7db7a"
      },
      "source": [
        "You can find the test video I used on YouTube: [Learn Data Science Tutorial - Full Course for Beginners](https://www.youtube.com/watch?v=ua-CiDNNj30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2aece1b",
      "metadata": {
        "id": "b2aece1b",
        "outputId": "d6fdfe93-e28c-4e48-c683-a3ef7b692db5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "c:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "c:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "'../datasets/videos/learn-data-science-tutorial.mp4' not found",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:904\u001b[39m, in \u001b[36mffmpeg_parse_infos\u001b[39m\u001b[34m(filename, check_duration, fps_source, decode_file, print_infos)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:606\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m video_file_path = \u001b[33m\"\u001b[39m\u001b[33m../datasets/videos/learn-data-science-tutorial.mp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m image_output_folder = \u001b[33m\"\u001b[39m\u001b[33m../datasets/videos/video_extracted_images\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m clip = \u001b[43mVideoFileClip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# create a list of timestamps from which we want to extract a frame\u001b[39;00m\n\u001b[32m     13\u001b[39m time_step = \u001b[32m10\u001b[39m  \u001b[38;5;66;03m# time in seconds\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-108>:2\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, filename, decode_file, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, fps_source, pixel_format, is_mask)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\moviepy\\decorators.py:102\u001b[39m, in \u001b[36mwrapper\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;129m@decorator\u001b[39m.decorator\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_mask_if_none\u001b[39m(f, clip, *a, **k):\n\u001b[32m    101\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" Add a mask to the clip if there is none. \"\"\"\u001b[39;00m        \n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clip.mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m         clip = clip.add_mask()\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, *a, **k)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\moviepy\\video\\io\\VideoFileClip.py:109\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, filename, decode_file, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, fps_source, pixel_format, is_mask)\u001b[39m\n\u001b[32m    106\u001b[39m     mask_mf = \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mself\u001b[39m.reader.get_frame(t)[:,:,\u001b[32m3\u001b[39m]/\u001b[32m255.0\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mself\u001b[39m.mask = (VideoClip(ismask=\u001b[38;5;28;01mTrue\u001b[39;00m, make_frame=mask_mf)\n\u001b[32m    108\u001b[39m                  .set_duration(\u001b[38;5;28mself\u001b[39m.duration))\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28mself\u001b[39m.mask.fps = \u001b[38;5;28mself\u001b[39m.fps\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.make_frame = \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mself\u001b[39m.reader.get_frame(t)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:35\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, filename, decode_file, print_infos, bufsize, pixel_format, check_duration, target_resolution, resize_algo, fps_source)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mself\u001b[39m.filename = filename\n\u001b[32m     34\u001b[39m \u001b[38;5;28mself\u001b[39m.proc = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[32m     36\u001b[39m                            fps_source)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m.fps = infos[\u001b[33m'\u001b[39m\u001b[33mvideo_fps\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m.size = infos[\u001b[33m'\u001b[39m\u001b[33mvideo_size\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:909\u001b[39m, in \u001b[36mffmpeg_parse_infos\u001b[39m\u001b[34m(filename, check_duration, fps_source, decode_file, print_infos)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "\u001b[31mFileNotFoundError\u001b[39m: '../datasets/videos/learn-data-science-tutorial.mp4' not found"
          ]
        }
      ],
      "source": [
        "\n",
        "# tag::load_video_and_extract_frames[]\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from moviepy import VideoFileClip, TextClip, CompositeVideoClip\n",
        "\n",
        "video_file_path = \"../datasets/videos/learn-data-science-tutorial.mp4\"\n",
        "image_output_folder = \"../datasets/videos/video_extracted_images\"\n",
        "\n",
        "clip = VideoFileClip(video_file_path)\n",
        "\n",
        "# create a list of timestamps from which we want to extract a frame\n",
        "time_step = 10  # time in seconds\n",
        "timestamps = list(range(0, int(clip.duration) - time_step, time_step))\n",
        "\n",
        "# for each timestamp extract a frame\n",
        "for timestamp in timestamps:\n",
        "    frame_image_path = os.path.join(image_output_folder, f\"frame_{timestamp}.png\")\n",
        "    clip.save_frame(frame_image_path, t=timestamp)\n",
        "# end::load_video_and_extract_frames[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75224617",
      "metadata": {
        "id": "75224617",
        "outputId": "a079b80b-4a6e-4790-d143-985fc9f3de88"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'timestamps' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# tag::video_to_audio[]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# for each timestamp extract the audio sequence and save it to a .mp3 file\u001b[39;00m\n\u001b[32m      3\u001b[39m audio_output_folder = \u001b[33m\"\u001b[39m\u001b[33m../datasets/videos/video_extracted_audio\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m timestamp \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtimestamps\u001b[49m:\n\u001b[32m      6\u001b[39m     audio_clip = clip.subclip(timestamp, timestamp + time_step).audio\n\u001b[32m      7\u001b[39m     output_audio_path = os.path.join(audio_output_folder, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maudio_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mp3\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'timestamps' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# tag::video_to_audio[]\n",
        "# for each timestamp extract the audio sequence and save it to a .mp3 file\n",
        "audio_output_folder = \"../datasets/videos/video_extracted_audio\"\n",
        "\n",
        "for timestamp in timestamps:\n",
        "    audio_clip = clip.subclip(timestamp, timestamp + time_step).audio\n",
        "    output_audio_path = os.path.join(audio_output_folder, f\"audio_{timestamp}.mp3\")\n",
        "    audio_clip.write_audiofile(output_audio_path)\n",
        "\n",
        "# end::video_to_audio[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982d1de4",
      "metadata": {
        "id": "982d1de4",
        "outputId": "ff562832-eb38-434e-913a-c9efe08292ae"
      },
      "outputs": [
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Invalid file format. Supported formats: ['flac', 'm4a', 'mp3', 'mp4', 'mpeg', 'mpga', 'oga', 'ogg', 'wav', 'webm']\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m     absolut_path_audio_file = os.path.join(audio_output_folder, audio_file)\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Use the function audio_to_text to convert the audio to text\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[43maudio_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mabsolut_path_audio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# end::audio_to_text[]\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36maudio_to_text\u001b[39m\u001b[34m(audio_path)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Open and read the audio file\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(audio_path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m audio_file:\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Transcribe\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     transcription = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscriptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhisper-1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio_file\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# save the transcription to a text file\u001b[39;00m\n\u001b[32m     28\u001b[39m text_file_path = audio_path.replace(\u001b[33m\"\u001b[39m\u001b[33m.mp3\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\openai\\resources\\audio\\transcriptions.py:333\u001b[39m, in \u001b[36mTranscriptions.create\u001b[39m\u001b[34m(self, file, model, chunking_strategy, include, language, prompt, response_format, stream, temperature, timestamp_granularities, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# It should be noted that the actual Content-Type header that will be\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# sent to the server will contain a `boundary` parameter, e.g.\u001b[39;00m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# multipart/form-data; boundary=---abc--\u001b[39;00m\n\u001b[32m    332\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmultipart/form-data\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/audio/transcriptions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtranscription_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTranscriptionCreateParamsStreaming\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtranscription_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTranscriptionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_get_response_format_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTranscriptionStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_ch01_loading_data\\Lib\\site-packages\\openai\\_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid file format. Supported formats: ['flac', 'm4a', 'mp3', 'mp4', 'mpeg', 'mpga', 'oga', 'ogg', 'wav', 'webm']\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
          ]
        }
      ],
      "source": [
        "\n",
        "# tag::audio_to_text[]\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "def audio_to_text(audio_path):\n",
        "    \"\"\"\n",
        "    Convert audio to text using OpenAI's Whisper model.\n",
        "\n",
        "    Parameters:\n",
        "    audio_path (str): The path to the audio file.\n",
        "\n",
        "    Returns:\n",
        "    str: The text recognized from the audio.\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialize the OpenAI client with your API key\n",
        "\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    # Open and read the audio file\n",
        "    with open(audio_path, \"rb\") as audio_file:\n",
        "        # Transcribe\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\", file=audio_file\n",
        "        )\n",
        "\n",
        "    # save the transcription to a text file\n",
        "    text_file_path = audio_path.replace(\".mp3\", \".txt\")\n",
        "    with open(text_file_path, \"w\") as text_file:\n",
        "        text_file.write(transcription.text)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# List all files in folder audio_output_folder\n",
        "audio_files = os.listdir(audio_output_folder)\n",
        "\n",
        "for audio_file in audio_files:\n",
        "    absolut_path_audio_file = os.path.join(audio_output_folder, audio_file)\n",
        "    # Use the function audio_to_text to convert the audio to text\n",
        "    audio_to_text(audio_path=absolut_path_audio_file)\n",
        "# end::audio_to_text[]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_ch01_loading_data",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}