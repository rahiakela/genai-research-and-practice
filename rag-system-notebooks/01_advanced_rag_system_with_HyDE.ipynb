{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/rag-system-notebooks/01_advanced_rag_system_with_HyDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "eF9EirgliuPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain huggingface_hub chromadb tiktoken faiss-cpu\n",
        "!pip -q install sentence_transformers\n",
        "!pip -q install -U FlagEmbedding"
      ],
      "metadata": {
        "id": "8jCFrv_Uivim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can you download the blog posts from here https://www.dropbox.com/scl/fi/ulbt145sthizf2nazey49/langchain_blog_posts.zip?rlkey=9unhw0vukhlwacahmpnk5m591&dl=0\n",
        "!wget https://github.com/rahiakela/genai-research-and-practice/raw/main/rag-system-notebooks/langchain_blog_posts.zip\n",
        "\n",
        "!mkdir -p blog_posts\n",
        "!unzip -q langchain_blog_posts.zip -d blog_posts"
      ],
      "metadata": {
        "id": "ZxaKqjAAx6aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import LLMChain, HypotheticalDocumentEmbedder\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "import langchain"
      ],
      "metadata": {
        "id": "Ln5WZyf05DVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BGE Embeddings**"
      ],
      "metadata": {
        "id": "378x9zFvp-Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "CH1VEgdfp9TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Liama 2**"
      ],
      "metadata": {
        "id": "i4MfN69OQTzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off"
      ],
      "metadata": {
        "id": "Gat-S7rwZ0cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "MODEL_NAME = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ],
      "metadata": {
        "id": "imYAolD8Zlo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##HyDE"
      ],
      "metadata": {
        "id": "MZOienCDX1Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load with `web_search` prompt\n",
        "embeddings = HypotheticalDocumentEmbedder.from_llm(llm,\n",
        "                                                   bge_embeddings,\n",
        "                                                   prompt_key=\"web_search\"\n",
        "                                                   )"
      ],
      "metadata": {
        "id": "5IeFT234go33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.llm_chain.prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjfCY_1MXOKm",
        "outputId": "fc315423-f3e9-474b-bcb9-71e3410721ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['QUESTION'], template='Please write a passage to answer the question \\nQuestion: {QUESTION}\\nPassage:')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.debug = True"
      ],
      "metadata": {
        "id": "mPDEUebYbhWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can use it as any embedding class!\n",
        "result = embeddings.embed_query(\"What items does McDonalds make?\")"
      ],
      "metadata": {
        "id": "9cZfWCEzg1Rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e3a920-21b6-44a7-a1ca-372b2b6ea1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:HuggingFacePipeline] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Please write a passage to answer the question \\nQuestion: What items does McDonalds make?\\nPassage:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:HuggingFacePipeline] [211.80s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"\\nMcDonald's is one of the largest fast-food chains in the world, known for its signature menu items such as the Big Mac sandwich, Chicken McNuggets, and French Fries. The company also offers a variety of drinks, including soft drinks, milkshakes, and coffee. In addition to these classic items, McDonald's frequently introduces limited-time offerings and seasonal specials to keep their menu fresh and exciting for customers. Whether you're looking for a quick breakfast on-the-go or a satisfying meal for dinner, McDonald's has something for everyone.\",\n",
            "        \"generation_info\": null,\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# result"
      ],
      "metadata": {
        "id": "35t2t5rMg7eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple generations\n",
        "We can also generate multiple documents and then combine the embeddings for those. By default, we combine those by taking the average. We can do this by changing the LLM we use to generate documents to return multiple things."
      ],
      "metadata": {
        "id": "I35V_-_2hnND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_llm = OpenAI(n=4, best_of=4)"
      ],
      "metadata": {
        "id": "8xwiuCtog5i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HypotheticalDocumentEmbedder.from_llm(\n",
        "    multi_llm, bge_embeddings, \"web_search\"\n",
        ")"
      ],
      "metadata": {
        "id": "eCrTReegh0_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = embeddings.embed_query(\"What is McDonalds best selling item?\")"
      ],
      "metadata": {
        "id": "KeppFREph3II",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0597046-71f2-4305-e0f2-cfb5b79941df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:OpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Please write a passage to answer the question \\nQuestion: What is McDonalds best selling item?\\nPassage:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:OpenAI] [4.03s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \" McDonalds is one of the most popular fast food restaurants in the world with its iconic golden arches logo. Its menu includes a variety of items, but one item stands out as the best seller. The Big Mac, introduced in 1968 and now one of the most iconic items in McDonalds history, is the best selling item on the menu. It is a two-patty hamburger made with a special sauce, lettuce, cheese, pickles, and onions on a sesame seed bun. The Big Mac is a classic that has stood the test of time and continues to be a favorite among customers. In 2020, McDonalds sold over 1 billion Big Macs worldwide, making it the clear best selling item in the McDonalds lineup.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"text\": \"\\nMcDonald's best selling item is undoubtedly the Big Mac. This iconic burger has been a staple of the McDonald's menu for decades and has become the unofficial mascot for the fast food chain. The Big Mac is made up of two all-beef patties, a slice of cheese, lettuce, onions, pickles, and of course, the special Big Mac sauce. It's the combination of these ingredients that make the Big Mac so popular. It's a classic burger that will never go out of style. In fact, the Big Mac is so popular that it even has its own commercial jingle. It's no wonder why this burger is McDonald's most popular item!\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"text\": \" McDonalds is one of the most recognizable fast food chains in the world. They offer a variety of menu items, however, their most popular item is the Big Mac. This iconic burger consists of two beef patties, special sauce, lettuce, cheese, pickles, and onions, all sandwiched between a three part sesame seed bun. It has been the number one seller since it was first introduced in 1968 and has become an international favorite. It is so popular that McDonald's has even created variations of the Big Mac in different countries, such as the Mega Mac in Japan which has four beef patties and the Maharaja Mac in India which is made with a chicken patty. The Big Mac is a staple of the McDonald's menu and will likely remain the top-selling item for years to come.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"text\": \" McDonalds is well known for their signature Big Mac, which is one of the most popular and best selling items on their menu. The Big Mac is made with two beef patties, special sauce, lettuce, cheese, pickles, and onions, all served on a sesame seed bun. The iconic burger has been a staple of the McDonalds menu since its introduction in 1968 and is still one of the most popular items on the menu today. Other popular items on the McDonalds menu include the Quarter Pounder, Chicken McNuggets, and fries.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 24,\n",
            "      \"completion_tokens\": 564,\n",
            "      \"total_tokens\": 588\n",
            "    },\n",
            "    \"model_name\": \"text-davinci-003\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using our own prompts\n",
        "Besides using preconfigured prompts, we can also easily construct our own prompts and use those in the LLMChain that is generating the documents. This can be useful if we know the domain our queries will be in, as we can condition the prompt to generate text more similar to that.\n",
        "\n",
        "In the example below, let's condition it to generate text about a state of the union address (because we will use that in the next example)."
      ],
      "metadata": {
        "id": "gOBLGPKqh-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Please answer the user's question as a single food item\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "hl6n-iIWh5S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HypotheticalDocumentEmbedder(\n",
        "    llm_chain=llm_chain,\n",
        "    base_embeddings=bge_embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "XySX7gRZiMEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = embeddings.embed_query(\n",
        "    \"What is is McDonalds best selling item?\"\n",
        ")"
      ],
      "metadata": {
        "id": "Y0Hz5eKXiTvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6e7de0-d0c0-45e2-d8e7-2dac9937fc4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:HuggingFacePipeline] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Please answer the user's question as a single food item\\nQuestion: What is is McDonalds best selling item?\\nAnswer:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:HuggingFacePipeline] [7.67s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \" The Big Mac.\",\n",
            "        \"generation_info\": null,\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "S4azcFwqbWLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using HyDE\n",
        "\n",
        "Now that we have HyDE, we can use it as we would any other embedding class! Here is using it to find similar passages in the state of the union example."
      ],
      "metadata": {
        "id": "nsVi5ucMilq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# with open(\"../../state_of_the_union.txt\") as f:\n",
        "#     state_of_the_union = f.read()\n",
        "\n",
        "loaders = [\n",
        "    TextLoader('/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'),\n",
        "    TextLoader('/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'),\n",
        "    TextLoader('/content/blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'),\n",
        "]\n",
        "docs = []\n",
        "for l in loaders:\n",
        "    docs.extend(l.load())\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "texts = text_splitter.split_documents(docs) #split_text"
      ],
      "metadata": {
        "id": "hWTWUBcBiWf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "id": "HYulGwZsc4F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3TTMYzSdbZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Please answer the user's question as related to Large Language Models\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "22IRw0uHdb_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HypotheticalDocumentEmbedder(\n",
        "    llm_chain=llm_chain,\n",
        "    base_embeddings=bge_embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "A-xmDGJEdb_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "query = \"What are chat loaders?\"\n",
        "docs = docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "fVwwG28ki5-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1f060c-6eeb-4c15-d0f7-377c22db0ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:OpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Please answer the user's question as related to Large Language Models\\nQuestion: What are chat loaders?\\nAnswer:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:OpenAI] [1.17s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \" Chat loaders are software tools used to load large language models into chatbot applications. They help to optimize the performance of the chatbot by enabling it to access large language models quickly and efficiently.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 24,\n",
            "      \"completion_tokens\": 39,\n",
            "      \"total_tokens\": 63\n",
            "    },\n",
            "    \"model_name\": \"text-davinci-003\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "0pNCA7hxjK4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81206283-7c4d-42e3-c866-4ba78e464c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://blog.langchain.dev/chat-loaders-finetune-a-chatmodel-in-your-voice/\n",
            "Title: Chat Loaders: Fine-tune a ChatModel in your Voice\n",
            "\n",
            "Summary\n",
            "\n",
            "We are adding a new integration type, ChatLoaders, to make it easier to fine-tune models on your own unique writing style. These utilities help convert data from popular messaging platforms to chat messages compatible with fine-tuning formats like that supported by OpenAI.\n",
            "\n",
            "Thank you to Greg Kamradt for Misbah Syed for their thought leadership on this.\n",
            "\n",
            "Important Links:\n",
            "\n",
            "Context\n",
            "\n",
            "On Tuesday, OpenAI announced improved fine-tuning support, extending the service to larger chat models like GPT-3.5-turbo. This enables anyone to customize these larger, more capable models for their own use cases. They also teased support for fine-tuning GPT-4 later this year.\n",
            "\n",
            "While fine-tuning is typically not advised for teaching an LLM substantially new knowledge or for factual recall; it is good for style transfer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5Dht92NX1Gt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}