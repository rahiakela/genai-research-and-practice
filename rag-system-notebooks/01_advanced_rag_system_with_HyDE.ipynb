{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/rag-system-notebooks/01_advanced_rag_system_with_HyDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRYSu48huSUW"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain huggingface_hub chromadb tiktoken faiss-cpu\n",
        "!pip -q install sentence_transformers\n",
        "!pip -q install -U FlagEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p blog_posts\n",
        "!unzip -q /content/langchain_blog_posts.zip -d blog_posts"
      ],
      "metadata": {
        "id": "ZxaKqjAAx6aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothetical Document Embeddings (HyDE)\n",
        "\n",
        "modified from - https://github.com/langchain-ai/langchain/tree/master/cookbook\n",
        "\n",
        "HyDE creates a \"Hypothetical\" answer with the LLM and then embeds that for search\n",
        "\n",
        "HyDE = Base Embedding model+ LLM Chain (with prompts)"
      ],
      "metadata": {
        "id": "VOVBrYQLga-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import LLMChain, HypotheticalDocumentEmbedder\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "import langchain"
      ],
      "metadata": {
        "id": "Ln5WZyf05DVh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BGE Embeddings"
      ],
      "metadata": {
        "id": "378x9zFvp-Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "CH1VEgdfp9TF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Liama 2"
      ],
      "metadata": {
        "id": "i4MfN69OQTzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off"
      ],
      "metadata": {
        "id": "Gat-S7rwZ0cZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "MODEL_NAME = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ],
      "metadata": {
        "id": "imYAolD8Zlo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##HyDE"
      ],
      "metadata": {
        "id": "MZOienCDX1Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load with `web_search` prompt\n",
        "embeddings = HypotheticalDocumentEmbedder.from_llm(llm,\n",
        "                                                   bge_embeddings,\n",
        "                                                   prompt_key=\"web_search\"\n",
        "                                                   )"
      ],
      "metadata": {
        "id": "5IeFT234go33"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.llm_chain.prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjfCY_1MXOKm",
        "outputId": "fc315423-f3e9-474b-bcb9-71e3410721ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['QUESTION'], template='Please write a passage to answer the question \\nQuestion: {QUESTION}\\nPassage:')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.debug = True"
      ],
      "metadata": {
        "id": "mPDEUebYbhWg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can use it as any embedding class!\n",
        "result = embeddings.embed_query(\"What items does McDonalds make?\")"
      ],
      "metadata": {
        "id": "9cZfWCEzg1Rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e3a920-21b6-44a7-a1ca-372b2b6ea1b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:HuggingFacePipeline] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Please write a passage to answer the question \\nQuestion: What items does McDonalds make?\\nPassage:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:HuggingFacePipeline] [211.80s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"\\nMcDonald's is one of the largest fast-food chains in the world, known for its signature menu items such as the Big Mac sandwich, Chicken McNuggets, and French Fries. The company also offers a variety of drinks, including soft drinks, milkshakes, and coffee. In addition to these classic items, McDonald's frequently introduces limited-time offerings and seasonal specials to keep their menu fresh and exciting for customers. Whether you're looking for a quick breakfast on-the-go or a satisfying meal for dinner, McDonald's has something for everyone.\",\n",
            "        \"generation_info\": null,\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# result"
      ],
      "metadata": {
        "id": "35t2t5rMg7eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple generations\n",
        "We can also generate multiple documents and then combine the embeddings for those. By default, we combine those by taking the average. We can do this by changing the LLM we use to generate documents to return multiple things."
      ],
      "metadata": {
        "id": "I35V_-_2hnND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_llm = OpenAI(n=4, best_of=4)"
      ],
      "metadata": {
        "id": "8xwiuCtog5i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HypotheticalDocumentEmbedder.from_llm(\n",
        "    multi_llm, bge_embeddings, \"web_search\"\n",
        ")"
      ],
      "metadata": {
        "id": "eCrTReegh0_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = embeddings.embed_query(\"What is McDonalds best selling item?\")"
      ],
      "metadata": {
        "id": "KeppFREph3II",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0597046-71f2-4305-e0f2-cfb5b79941df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:OpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Please write a passage to answer the question \\nQuestion: What is McDonalds best selling item?\\nPassage:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:OpenAI] [4.03s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \" McDonalds is one of the most popular fast food restaurants in the world with its iconic golden arches logo. Its menu includes a variety of items, but one item stands out as the best seller. The Big Mac, introduced in 1968 and now one of the most iconic items in McDonalds history, is the best selling item on the menu. It is a two-patty hamburger made with a special sauce, lettuce, cheese, pickles, and onions on a sesame seed bun. The Big Mac is a classic that has stood the test of time and continues to be a favorite among customers. In 2020, McDonalds sold over 1 billion Big Macs worldwide, making it the clear best selling item in the McDonalds lineup.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"text\": \"\\nMcDonald's best selling item is undoubtedly the Big Mac. This iconic burger has been a staple of the McDonald's menu for decades and has become the unofficial mascot for the fast food chain. The Big Mac is made up of two all-beef patties, a slice of cheese, lettuce, onions, pickles, and of course, the special Big Mac sauce. It's the combination of these ingredients that make the Big Mac so popular. It's a classic burger that will never go out of style. In fact, the Big Mac is so popular that it even has its own commercial jingle. It's no wonder why this burger is McDonald's most popular item!\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"text\": \" McDonalds is one of the most recognizable fast food chains in the world. They offer a variety of menu items, however, their most popular item is the Big Mac. This iconic burger consists of two beef patties, special sauce, lettuce, cheese, pickles, and onions, all sandwiched between a three part sesame seed bun. It has been the number one seller since it was first introduced in 1968 and has become an international favorite. It is so popular that McDonald's has even created variations of the Big Mac in different countries, such as the Mega Mac in Japan which has four beef patties and the Maharaja Mac in India which is made with a chicken patty. The Big Mac is a staple of the McDonald's menu and will likely remain the top-selling item for years to come.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"text\": \" McDonalds is well known for their signature Big Mac, which is one of the most popular and best selling items on their menu. The Big Mac is made with two beef patties, special sauce, lettuce, cheese, pickles, and onions, all served on a sesame seed bun. The iconic burger has been a staple of the McDonalds menu since its introduction in 1968 and is still one of the most popular items on the menu today. Other popular items on the McDonalds menu include the Quarter Pounder, Chicken McNuggets, and fries.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 24,\n",
            "      \"completion_tokens\": 564,\n",
            "      \"total_tokens\": 588\n",
            "    },\n",
            "    \"model_name\": \"text-davinci-003\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using our own prompts\n",
        "Besides using preconfigured prompts, we can also easily construct our own prompts and use those in the LLMChain that is generating the documents. This can be useful if we know the domain our queries will be in, as we can condition the prompt to generate text more similar to that.\n",
        "\n",
        "In the example below, let's condition it to generate text about a state of the union address (because we will use that in the next example)."
      ],
      "metadata": {
        "id": "gOBLGPKqh-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Please answer the user's question as a single food item\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "hl6n-iIWh5S0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HypotheticalDocumentEmbedder(\n",
        "    llm_chain=llm_chain,\n",
        "    base_embeddings=bge_embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "XySX7gRZiMEO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = embeddings.embed_query(\n",
        "    \"What is is McDonalds best selling item?\"\n",
        ")"
      ],
      "metadata": {
        "id": "Y0Hz5eKXiTvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6e7de0-d0c0-45e2-d8e7-2dac9937fc4c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:HuggingFacePipeline] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Please answer the user's question as a single food item\\nQuestion: What is is McDonalds best selling item?\\nAnswer:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:HuggingFacePipeline] [7.67s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \" The Big Mac.\",\n",
            "        \"generation_info\": null,\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "S4azcFwqbWLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using HyDE\n",
        "\n",
        "Now that we have HyDE, we can use it as we would any other embedding class! Here is using it to find similar passages in the state of the union example."
      ],
      "metadata": {
        "id": "nsVi5ucMilq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# with open(\"../../state_of_the_union.txt\") as f:\n",
        "#     state_of_the_union = f.read()\n",
        "\n",
        "loaders = [\n",
        "    TextLoader('/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'),\n",
        "    TextLoader('/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'),\n",
        "    TextLoader('/content/blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'),\n",
        "]\n",
        "docs = []\n",
        "for l in loaders:\n",
        "    docs.extend(l.load())\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "texts = text_splitter.split_documents(docs) #split_text"
      ],
      "metadata": {
        "id": "hWTWUBcBiWf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYulGwZsc4F7",
        "outputId": "c983d6d3-fa14-461a-870b-0660f9d0fc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='URL: https://blog.langchain.dev/announcing-langsmith/\\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\\n\\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\\n\\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “not enough tinkering happening.” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='The blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\\n\\nToday, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs.\\n\\nLangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\\n\\nHow did we get here?', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='Given the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too):\\n\\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\\n\\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\\n\\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\\n\\nTracking token usage\\n\\nManaging costs\\n\\nTracking (and debugging) latency\\n\\nNot having a good dataset to evaluate their application over\\n\\nNot having good metrics with which to evaluate their application\\n\\nUnderstanding how users are interacting with the product', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='All of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\\n\\nLangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\n\\nDebugging\\n\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues.', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='We’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general.\\n\\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake.\\n\\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure.', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content=\"“We are proud of being one of the early LangChain design partners and users of LangSmith,” Said Dan Sack, Managing Director and Partner at BCG. “The use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith's ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\", metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='In another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\\n\\nTesting\\n\\nOne of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets.', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='The first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='Today, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\\n\\nIn parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach.\\n\\nEvaluating\\n\\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='We are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably.\\n\\nMonitoring', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='While debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n\\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='“Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\\n\\nA unified platform\\n\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n\\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena.\\n\\n“Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.”', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content='We can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\\n\\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\\n\\nWe can’t wait to see what you build.', metadata={'source': '/content/blog_posts/blog.langchain.dev_announcing-langsmith_.txt'}),\n",
              " Document(page_content=\"URL: https://blog.langchain.dev/benchmarking-question-answering-over-csv-data/\\nTitle: Benchmarking Question/Answering Over CSV Data\\n\\nThis is a bit of a longer post. It's a deep dive on question-answering over tabular data. We discuss (and use) CSV data in this post, but a lot of the same ideas apply to SQL data. It covers:\\n\\nBackground Motivation: why this is an interesting task\\n\\nInitial Application: how we set up a simple Streamlit app in order to gather a good distribution of real questions\\n\\nInitial Solution: our initial solution and some conceptual considerations\\n\\nDebugging with LangSmith: what we saw people asking, and what issues our initial solution had\\n\\nEvaluation Setup: how we evaluated solutions\\n\\nImproved Solution: the final improved solution we arrived at\\n\\nAs a sneak preview, the improved solution we arrived at was a custom agent that used OpenAI functions and had access to two tools: a Python REPL and a retriever.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"We've open-sourced everything - the app we used to gather feedback, the dataset, the eval script - at this repo. We've also made a YouTube video walking through the content in this blog, if that's more your style.\\n\\nBackground Motivation\\n\\nThere's a pretty standard recipe for question over text data at this point. On the other hand, one area where we've heard consistent asks for improvement is with regards to tabular (CSV) data. Lots of enterprise data is contained in CSVs, and exposing a natural language interface over it can enable easy insights. The problem is that it's far less clear how to accomplish this.\\n\\nA few weeks ago we decided to focus on this for a bit but quickly ran into an issue–we didn't really know what types of questions people expected to be able ask CSV data, and we didn't have any good way to evaluate this applications.\\n\\n💡 Evaluation of LLM applications is often hard because of a lack of data and a lack of metrics.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"In traditional machine learning you usually start with a dataset of inputs and outputs, and you use this to train and then evaluate your model. However, because LLMs are fantastic zero shot learners, it is now possible to use a prompt to quickly build an application based on just an idea, and no data. While this is incredibly powerful in terms of enabling developers to build new applications quickly, it leads to difficulty in evaluating because you lack that data. This is why we've built LangSmith in a way where constructing datasets is as easy as possible.\\n\\nLikewise, there's often not great metrics for evaluating LLM applications. The outputs are often natural language, and traditional NLP metrics like BLEU and ROUGE aren't great. But what is good at understanding natural language? LLMs! We're pretty bullish on LLM assisted evaluation and have invested in building a bunch of evaluators that use LLMs to do the evaluation.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"So how did we apply these ideas to our task of creating a better application for answering questions over tabular data? We'll dive into these in more detail in the next sections, but at a high level we:\\n\\nUsed LangSmith to flag interesting datapoints and used that to construct a dataset of examples\\n\\nUsed LLMs to evaluate correctness\\n\\nInitial Application\\n\\nFirst, we set about creating a dataset of questions and ground truth answers. Part of the issue here was didn't even know what type of questions people would want to ask of their tabular data. We could have made some educated guesses, or tried to generate synthetic questions to ask. But we wanted to optimize instead for real questions, as we also wanted to do a bit of exploration here into what types of questions real users would want to ask.\\n\\n💡 Before you launch an app it can be tough to guess how users may interact with it. Rather than guessing, one strategy is launch quickly and early and gather real data.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content='In order to do this we decided to spin up a quick demo application and put that out in the wild. We would then log actual user questions along with any feedback about the answers that they gave us. To gather feedback we added a simple \"thumps up\"/\"thumbs down\" button to the application. We would use LangSmith to monitor all the interactions and feedback, and then we would manually review the interactions and create a dataset consisting of any interesting ones. This is done easily from the LangSmith UI - there is an \"Add to Dataset\" button on all logs.', metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"There's also the question of what type of data we wanted to gather. We considered two approaches: (1) let users upload their own CSV and ask questions of that, (2) fix the CSV and gather questions over that. We opted for (2) for a few reasons. First - it would make it simpler for people to play around with, likely leading to more responses. Second - it would probably make it easier to evaluate. Third - we specifically wanted to be logging and looking at user questions, and we didn't want to do this over any confidential CSV that someone might upload. However, this does have several downsides. We would have to choose a CSV to use, and this CSV may not be representative of other CSVs - both in the size and shape of the data, as well as the questions people may want to ask of it.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"For our example application we chose the classic Titanic dataset - a record of all passengers on the Titanic and whether the survived, often used for example data science projects. We created this simple application in Streamlit, put it out in the world, and asked people to give feedback. You can view the hosted app here, and the source code here.\\n\\nThrough this, we gathered ~400 interactions. Of those, about 200 had some form of feedback. Using LangSmith, we drilled into datapoints with bad feedback (and some with good) and manually labeled them and added them to a dataset we created. We did this until we had about 50 datapoints.\\n\\nNow it was time to improve our system! Before talking about how we improved, let's first discuss (1) what the initial system was (2) what issues it had, and (3) how we would evaluate the system to measure any improvements.\\n\\nInitial Solution\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content='The Titanic dataset has a mix of columns in it. Some of them are numeric (age, number of siblings, fare), some of them are categorical (station embarked, cabin) and there\\'s one text column (name).\\n\\nWhile a person\\'s name isn\\'t super text heavy, it is still text heavy enough to cause some issues. For example if a question is asked about \"John Smith\", there a bunch of variants of how that name could be represented: Mr. John Smith (title), Smith, John (order), Jon Smith (typo), John Jacob Smith (middle name), etc. This can make it tricky to filter rows exactly by name, or even do lookups. Therefor, from the start we knew we had to include some more fuzzy based functionality. However, we also guessed that people would want to ask some questions about aggregations (\"who paid the most for their fare\") or the like, and so we probably need some functionality to do that.', metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"💡 Tabular data that contains text can be particularly tough to deal with, as retrieval is likely needed in some form, but pure retrieval probably isn't enough.\\n\\nRetrieval\\n\\nFor the natural language bit, we wanted to use a traditional retrieval system. We weren't going to get too fancy, so we just wanted to use a simple vectorstore and look up results based on cosine similarity with the input question.\\n\\nIn order to do this we needed to load a CSV into a vectorstore. We did this using the logic of our CSVLoader. What this does under the hood is:\\n\\nLoad each row as its own document Represent the text of each document as a list of Column: value pairs, each on their own line.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content='Digging into point (2) a bit more, there\\'s a few ways you could represent a row of CSV as a document. You could represent it as JSON, as a CSV, or - as we ended up doing - a formatted piece of text. Very concretely, if you had a CSV row with the following values: {\"col1\": \"foo\", \"col2\": \"bar\"} what this ends up looking like after you format it is:\\n\\ncol1: foo col2: bar\\n\\nWhile this may not seem all that interesting, a BIG part of LLM applications is proper data engineering to communicate data to the LLM most effectively. Anecdotally, we\\'ve found this representation of tabular (and also JSON) data to be most efficient when the values could contain textual values.\\n\\nQuery Language\\n\\nAside from retrieval, we also figured people would want to ask questions that required some type of query language. For example - \"who paid the most for their fare\". There are two approaches we considered here.', metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"First, we considered using a Python REPL and asking the language model to write code to help answer the user's question. This has the benefit of being very flexible. This also has the downside of maybe being TOO flexible - it could enable execution of arbitrary code.\\n\\nSecond, we considered using kork to give access to a predetermined set of functions. kork is a library that basically whitelists a set of functions that can be used. It's less general - you have to declare all functions that can be run - but it's safer.\\n\\nTo start, we went with kork . We weren't entirely sure about what people would ask, so we defined a few functions (filter, sum, contains) and gave it access to that.\\n\\nOur first solution ran retrieval and kork in parallel, and then combined the answers.\\n\\nDebugging with LangSmith\\n\\nPeople started asking questions and the feedback starting rolling in. Only about 1/3 of feedback was positive. What was going wrong? There was two main sources of errors:\\n\\nData Formatting\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content='A lot of the functions we wrote for kork would return a dataframe. This dataframe was then inserted into a prompt and passed to the language model. There was then a question of how that dataframe was formatted as a string to be passed to the language model.\\n\\nThis was important for answering questions like Who was in cabin C128 . The returned dataframe would have hopefully filtered to the correct row and be returning all relevant information. Before we launched the app, we tested questions like this and it was working fine. However, after we launched the app and started to look at the responses we noticed it was failing terribly at a large number of these types of questions.\\n\\nWe used LangSmith to inspect the traces to try to get a sense of what was going on. We could see that the correct query was being generated... but when that dataframe was passed into the prompt the formatting was being messed up. We expected it look something like:\\n\\nBut instead it was looking something like:', metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"After some more debugging, we discovered that how a dataframe is represented as string may change depending on what platform you are on. In this case, it was being represented differently locally compared to Streamlit cloud. After some more debugging, we figured out that we could fix that inconsistency by specifying some parameters:\\n\\npd.set_option('display.max_rows', 20) pd.set_option('display.max_columns', 20)\\n\\nDoing this fixed a lot of our issues! It also shows how LangSmith can be extremely helpful in debugging LLM issues. The main parts of bringing an LLM application from prototype to production are prompt engineering and data engineering. Understand what exactly the data looks like when you are passing it to an LLM is crucial for debugging performance issues. We've heard from several users of LangSmith who have found these types of data engineering issues only after using LangSmith to inspect the exact inputs to LLMs more carefully.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"💡 If data is not passed to the language model in a clear way, it will make it really tricky for the language model to reason about it correcting. Using LangSmith to make sure the final text looks reasonable, and debug any data processing steps, is a great way to catch any bugs here.\\n\\nLimited kork Functionality\\n\\nIt turns out the set of functions we gave to kork was not NEARLY enough to cover the long tail of questions users would ask.\\n\\nThere are two potential fixes to this. One, we could try to add more functions to kork . Second, we could revert to using a Python REPL.\\n\\nEvaluation Setup\\n\\nSo we've now constructed our dataset of real world examples. We've also done some manual debugging and identified some areas of errors and have some ideas for how to improve. How exactly do we go about measuring whether we've improved?\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content='For an example of why this is non-trivial, let\\'s consider the question Who is in cabin C128 . The correct answer in the CSV is Williams-Lambert, Mr. Fletcher Fellows . But there are a LOT of ways a language model could respond that should be considered \"correct\":\\n\\nMr. Fletcher Fellows Williams-Lambert\\n\\nThe person in cabin C128 was Mr. Fletcher Fellows Williams-Lambert.\\n\\nFletcher Williams-Lambert\\n\\nMr. Williams-Lambert was in that cabin\\n\\nIn order to properly evaluate these natural language answers... we turned to a language model. We decided to use our standard qa evaluator, which takes as input:\\n\\nThe input\\n\\nThe ground truth answer\\n\\nA predicted answer\\n\\nFrom there, it formats a prompt template with those values and passes that to a language model to get back a response.', metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"Even still, this is NOT perfect. For example, one of the questions we evaluated on was male to female ratio? . It's pretty unclear what the answer to that question should be. We had labelled the answer as There were 577 males and 314 females, for a ratio of 1.84 . In one test run, the language model responded The ratio of males to females in the dataframe is approximately 0.65 to 0.35. This means that there are about 65% males and 35% females . Our LLM evaluator marked that answer as INCORRECT, even though it probably likely correct.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content='Does this mean there is no use for an LLM evaluator? We do not believe so. Rather, we believe that LLM evaluators are still useful. For starters, they are markedly better than other \"general\" evaluation methods we\\'ve tried. Secondly, even if occasionally correct that can be totally fine if you\\'re not treating the grades as gospel. For example - don\\'t blindly accept the LLM scores, but rather treat them as indications of where it may be worth looking. Even if you still need to do human evaluation on some data points, using LLM assisted evaluation can help guide you to the most interesting datapoint to look at.\\n\\n💡 Evaluating LLM output using LLMs is NOT perfect, but we think this is currently the best available solution and are bullish on it in the long run.\\n\\nImproved Solution\\n\\nFinally, we arrive at the exciting part of the blog. Did we manage to improve our solution? And how did we do so?\\n\\nOur final solution is:\\n\\nAn agent powered by OpenAIFunctions ( OpenAIFunctionsAgent )\\n\\n) GPT-4', metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"Two tools: a Python REPL and a retriever\\n\\nA custom prompt with custom instructions on how to think about when to use the Python REPL vs the retriever\\n\\nThis provides several benefits. First, by giving it access to a Python REPL we give it the ability to do all sorts of queries and analysis. However, as we'll see in some of the comparisons below, the Python REPL can have issues when dealing with text data - in this case the Name column. That is where the retriever can come in handy.\\n\\n💡 Our final solution is an agent with two tools: a Python REPL and a retriever. This allows it to answer questions about the unstructured text, but also perform more traditional data analysis operations.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content='Note that we do include some instructions in the prompt specific to the Titanic dataset. Specifically, we tell it that it should try to use the retriever for the Name column and the Python REPL for most other things. We did this because with generic wording it was having some trouble reasoning about when to use it. This does mean that comparing to generic solutions (as we do below) is a bit unfair. As a follow up, we would love to see a more generic prompt presented that does not include dataset specific logic. However, we also believe that in order to really improve the performance of your application you will likely need to use a custom prompt and NOT rely on generic defaults.', metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"Now let's look at some results and compare to other methods. First, we compare to our standard Pandas Agent (using both GPT-3.5 as well as GPT-4). Next, we compare to PandasAI - one of the top open source libraries for interacting with Pandas DataFrames. A table of performance is below. Again, this is over 50 datapoints and some of the evaluations may not be 100% accurate, so we'll also present some concrete examples after the fact.\\n\\nNote: these were all run on LangSmith. We're working on making evaluation runs publicly sharable.\\n\\nThe Pandas Agent and PandasAI performed roughly the same. They struggled on questions involving people's names. For example, for the following question:\\n\\nHow many siblings does Carrie Johnston have?\\n\\nThe code generate is:\\n\\n# First, we need to find the row for Carrie Johnston carrie_johnston = df[df['Name'].str.contains('Carrie Johnston')] # Then, we can find the number of siblings she has num_siblings = carrie_johnston['SibSp'].values[0] num_siblings\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content='However, df[df[\\'Name\\'].str.contains(\\'Carrie Johnston\\')] does not return any rows because her name appears as Johnston, Miss. Catherine Helen \"Carrie\"\\n\\nLooking at the four example our custom agent gets wrong, we can see that a lot of the mistakes aren\\'t that bad.\\n\\nIn one case it filters based on age (the ground truth answer we added had no filtering - maybe there should have been?)\\n\\nIn another case it stops listing after 10 - this is actually because the DataFrame when printed out didn\\'t actually show the whole contents.\\n\\nIn a third case it just has a different interpretation of how to respond (but the facts look correct)\\n\\nAnd finally, it messes up because it uses the retriever to search for names, and the retriever is limited to four responses.\\n\\nConclusion', metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content=\"We're pretty satisfied with the final solution we arrived at - and most of the feedback had been positive as well. We're also pretty happy with the dataset we've put together and think it can be useful in evaluating these types of applications.\\n\\nAt the same time, we recognize that there is always room for improvements - on both fronts. The dataset can be improved/added to, the evaluators can likely be improved, and we're especially excited to see more solutions to this problem of question-answering over CSV data!\\n\\nWe've open-sourced everything - the app we used to gather feedback, the dataset, the eval script - at this repo.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'}),\n",
              " Document(page_content='URL: https://blog.langchain.dev/chat-loaders-finetune-a-chatmodel-in-your-voice/\\nTitle: Chat Loaders: Fine-tune a ChatModel in your Voice\\n\\nSummary\\n\\nWe are adding a new integration type, ChatLoaders, to make it easier to fine-tune models on your own unique writing style. These utilities help convert data from popular messaging platforms to chat messages compatible with fine-tuning formats like that supported by OpenAI.\\n\\nThank you to Greg Kamradt for Misbah Syed for their thought leadership on this.\\n\\nImportant Links:\\n\\nContext\\n\\nOn Tuesday, OpenAI announced improved fine-tuning support, extending the service to larger chat models like GPT-3.5-turbo. This enables anyone to customize these larger, more capable models for their own use cases. They also teased support for fine-tuning GPT-4 later this year.\\n\\nWhile fine-tuning is typically not advised for teaching an LLM substantially new knowledge or for factual recall; it is good for style transfer.', metadata={'source': '/content/blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'}),\n",
              " Document(page_content='We\\'ve had a lot of community members ask about the best ways to get ChatGPT to respond \"in your own voice\" - fine-tuning is an excellent way to do so!\\n\\nGreat people on Twitter like Greg Kamdrat have also been bullish on this use case:\\n\\nSetting the tone/style of the output is top of the list for me\\n\\nFine-tuning as a service to businesses that matches their tone\\n\\nCurrently investigating...will report back https://t.co/235WSJzxet pic.twitter.com/KDzMrdqccv — Greg Kamradt (@GregKamradt) August 22, 2023\\n\\nFine-tuning on your communications could be useful for a variety of applications, such as responding to customers in your brand\\'s voice, generating content that is more aware of your team\\'s unique jargon, chatting reliably in a target language, or just for fun!', metadata={'source': '/content/blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'}),\n",
              " Document(page_content=\"Why is this better than direct instructions? Style and tone can be hard to describe! Most of us don't write like ChatGPT, and it can sometimes be frustratingly difficult to get the LLM to consistently respond in a particular voice (especially over longer conversations).\\n\\nWhy is this better than few-shot examples? It can be challenging to capture your voice in only a few concise snippets! Fine-tuning lets you provide a larger number of examples the model can learn from without having to see them every time you want to query the model.\\n\\nChatLoaders\\n\\nAt LangChain, we want to make it as easy as possible for you to take advantage of this improved fine-tuning support. To make it simple to adapt a model to your voice, we're adding a new integration type: ChatLoaders .\", metadata={'source': '/content/blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'}),\n",
              " Document(page_content=\"These utilities take data exported from popular messaging platforms and convert them to LangChain message objects, which you can then easily convert platform-agnostic message formats, such as OpenAI, Llama 2, and others. This training data can be used directly for fine-tuning a model.\\n\\nWe've added loaders for the following popular messaging platforms so far:\\n\\nFacebook Messenger\\n\\nSlack\\n\\nTelegram\\n\\nWhatsApp\\n\\nWe have also added a recipe on how to do so for Discord and Twitter (using Apify) and plan to integrate additional chat loaders in the near future. If you have a favorite messaging platform you'd like to support, we'd love to help you land a PR!\\n\\nTo get you started, we've added an end-to-end example notebook to the LangChain documentation showing how to fine-tune gpt-3.5-turbo (the model behind ChatGPT) on an example set of Facebook messages.\\n\\n❗ Please ensure all participants of your conversations support the decision to train a model on the chat data before proceeding.\", metadata={'source': '/content/blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'}),\n",
              " Document(page_content='Once you have your fine-tuned model, you can use the model name directly in LangChain\\'s ChatOpenAI class:\\n\\nfrom langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=\"ft:gpt-3.5-turbo-0613:{openaiOrg}::{modelId}\") llm.predict(\"What classes are you taking this year?\")\\n\\nThen you can plug this into any other LangChain component!\\n\\nEnd-to-End Example\\n\\nWe\\'ve also created an end-to-end example of finetuning a model based on Elon Musk\\'s tweets. This uses Apify to load data. Note that it\\'s less than 100 examples so results may not be the most amazing they could be.\\n\\nWe open-sourced this example at the GitHub repo here. We also hosted it on Streamlit app so you can easily play around with it here.\\n\\nWebinar\\n\\nThere is a lot more to discuss on this topic. What types of messages are best for finetuning? What others sources of data exist for this? How many points do you need?\\n\\nWe\\'ll be discussing this and more next week in a webinar with Greg Kamradt. Come join!\\n\\nConclusion', metadata={'source': '/content/blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'}),\n",
              " Document(page_content=\"We're excited to see all the creative applications fine-tuning unlocks. We have implemented a few ChatLoaders already, but we need your help to make it easier to create your own personalized model. Help us create more ChatLoaders!\", metadata={'source': '/content/blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3TTMYzSdbZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Please answer the user's question as related to Large Language Models\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "22IRw0uHdb_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HypotheticalDocumentEmbedder(\n",
        "    llm_chain=llm_chain,\n",
        "    base_embeddings=bge_embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "A-xmDGJEdb_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "query = \"What are chat loaders?\"\n",
        "docs = docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "fVwwG28ki5-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1f060c-6eeb-4c15-d0f7-377c22db0ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:OpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Please answer the user's question as related to Large Language Models\\nQuestion: What are chat loaders?\\nAnswer:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:OpenAI] [1.17s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \" Chat loaders are software tools used to load large language models into chatbot applications. They help to optimize the performance of the chatbot by enabling it to access large language models quickly and efficiently.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 24,\n",
            "      \"completion_tokens\": 39,\n",
            "      \"total_tokens\": 63\n",
            "    },\n",
            "    \"model_name\": \"text-davinci-003\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "0pNCA7hxjK4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81206283-7c4d-42e3-c866-4ba78e464c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://blog.langchain.dev/chat-loaders-finetune-a-chatmodel-in-your-voice/\n",
            "Title: Chat Loaders: Fine-tune a ChatModel in your Voice\n",
            "\n",
            "Summary\n",
            "\n",
            "We are adding a new integration type, ChatLoaders, to make it easier to fine-tune models on your own unique writing style. These utilities help convert data from popular messaging platforms to chat messages compatible with fine-tuning formats like that supported by OpenAI.\n",
            "\n",
            "Thank you to Greg Kamradt for Misbah Syed for their thought leadership on this.\n",
            "\n",
            "Important Links:\n",
            "\n",
            "Context\n",
            "\n",
            "On Tuesday, OpenAI announced improved fine-tuning support, extending the service to larger chat models like GPT-3.5-turbo. This enables anyone to customize these larger, more capable models for their own use cases. They also teased support for fine-tuning GPT-4 later this year.\n",
            "\n",
            "While fine-tuning is typically not advised for teaching an LLM substantially new knowledge or for factual recall; it is good for style transfer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5Dht92NX1Gt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}