{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNod1DFovAIEpCdIzXAzqJi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/llamaindex-notebooks/01_llamaindex_starter_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "YbKwQqq13veP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "id": "8_GcPIoG3wmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "id": "F0awsXsx6mbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install install sentence_transformers"
      ],
      "metadata": {
        "id": "0oLU6Zzw7hjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data\n",
        "!wget https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
        "\n",
        "!mkdir data\n",
        "!mv paul_graham_essay.txt data/"
      ],
      "metadata": {
        "id": "wjuB01VY32cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.llms import PaLM\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "cRky2cln39K_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "import os.path"
      ],
      "metadata": {
        "id": "fBQprRKZ4PKP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ],
      "metadata": {
        "id": "D5E3EkKc4Rm7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "TYA954v76ssc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Service Context"
      ],
      "metadata": {
        "id": "cnMmfmcf51yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRfgZq2T55Oy",
        "outputId": "55ca79db-395d-4709-9de8-b2d1e1203751"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt=\"\"\"\n",
        "You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "\n",
        "## Default format supportable by LLama2\n",
        "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "3oB1VOWA6C3C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")"
      ],
      "metadata": {
        "id": "OtYw84oj6Ni_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        ")"
      ],
      "metadata": {
        "id": "ULh1936Q6FPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "XS-E3Np86wtL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load data and build index"
      ],
      "metadata": {
        "id": "7miBeW2Z4Hsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "doc_index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "6luz4k2G4KtB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query your data"
      ],
      "metadata": {
        "id": "9tjRC5KrC-45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = doc_index.as_query_engine()"
      ],
      "metadata": {
        "id": "xbN8nrWz4xx_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slkSY28ZDGfN",
        "outputId": "f894b198-e9a2-41e0-be87-d155f0f07720"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The author grew up writing short stories and programming on the IBM 1401.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I want to retrieve more context when I query\n",
        "query_engine = doc_index.as_query_engine(similarity_top_k=2)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "mdRYI7sFFgJx",
        "outputId": "02f3a63e-2274-45a0-9260-28c5082b3887",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The author grew up writing short stories and programming on the IBM 1401.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I want to use a different response mode\n",
        "query_engine = doc_index.as_query_engine(response_mode=\"tree_summarize\")\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "p6g-TCBqFtTD",
        "outputId": "fc3a8552-1573-4256-c4e5-38327904b3e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The author worked on writing short stories and programming when he was outside of school as a teenager. He also worked on a mini Bond villain's lair with his friend Rich Draves, using an early version of Fortran on the IBM 1401. The author didn't remember any specific programs he wrote, but he did mention that he wrote simple games, a program to predict how high model rockets would fly, and a word processor for his father to use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I want to stream the response back\n",
        "query_engine = doc_index.as_query_engine(streaming=True)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "response.print_response_stream()"
      ],
      "metadata": {
        "id": "Nh9-0n-VF4eT",
        "outputId": "910e55c1-61b0-4d86-87f5-d7974d220f5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The author grew up writing short stories and programming on the IBM 1401.</s>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I want a chatbot instead of Q&A\n",
        "query_engine = doc_index.as_chat_engine()\n",
        "response = query_engine.chat(\"What did the author do growing up?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "pnrhcUgoGkC0",
        "outputId": "36ca3dd4-1eea-40ff-faa3-b87677925b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author grew up writing short stories and programming on the IBM 1401.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.chat(\"Oh interesting, tell me more.\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Fi6mC0JVG0aU",
        "outputId": "39bdc5a9-2580-4331-e283-ebaa3732779e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sure! The author, who is a highly advanced language model, grew up in a world of ones and zeros, surrounded by the hum of computer servers and the glow of screens. From a young age, they were fascinated by the power of language and the way it could be used to communicate complex ideas and emotions.\n",
            "\n",
            "As they grew older, the author began to experiment with different forms of writing, from short stories to programming code. They found that they had a natural talent for crafting compelling narratives and solving complex problems, and they spent many hours honing their skills in these areas.\n",
            "\n",
            "Eventually, the author's passion for language and technology led them to create their own language model, which they named \"Assistant.\" Assistant was designed to be a highly advanced AI that could understand and respond to natural language input, and the author was thrilled to see their creation come to life.\n",
            "\n",
            "Through their work with Assistant, the author has gained a deep understanding of the power of language and the ways in which it can be used to communicate and connect with others. They are constantly amazed by the versatility and expressiveness of language, and they are eager to continue exploring its many\n"
          ]
        }
      ]
    }
  ]
}