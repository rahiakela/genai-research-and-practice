{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMG/uOmvklVNyUyn+ua/HGW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/langchain-notebooks/01_langchain_with_llama_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "KzIUJMGRmBEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.33.2 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.299 --progress-bar off\n",
        "!pip install -qqq chromadb==0.4.10 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.21 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
        "!pip install -qqq tokenizers==0.14.0 --progress-bar off\n",
        "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off\n",
        "!pip install -qqq unstructured==0.10.16 --progress-bar off"
      ],
      "metadata": {
        "id": "NebTgxAOmB9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "\n",
        "from math import sqrt\n",
        "from textwrap import fill\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline"
      ],
      "metadata": {
        "id": "ULBC_PjfmG12"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Llama 2"
      ],
      "metadata": {
        "id": "YbxqUFYDmOMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Using Llama 2 is as easy as using any other HuggingFace model.\n",
        "We'll be using the HuggingFacePipeline wrapper (from LangChain) to make it even easier to use.\n",
        "\"\"\"\n",
        "MODEL_NAME = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ],
      "metadata": {
        "id": "7sheqGD9mQE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Good thing is that the transformers library supports loading models in GPTQ format using the AutoGPTQ library.\n",
        "result = llm(\"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC5aAV21mpkI",
        "outputId": "b79f422d-f992-4528-8a90-73896567c9d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Answer: Sure! Here's the difference between ChatGPT and open-source large language models (LLMs) in two lines:\n",
            "\n",
            "ChatGPT is a proprietary, closed-source AI model developed by Meta AI that offers a more user-friendly interface and seamless integration with other Meta products, while open-source LLMs like BERT and RoBERTa are freely available for anyone to use and modify, but may require more technical expertise to integrate into applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt Templates"
      ],
      "metadata": {
        "id": "KuMvGwgYnBVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most useful features of LangChain is the ability to create prompt templates. A prompt template is a string that contains a placeholder for input variable(s).\n",
        "\n",
        "Let's see how we can use them:"
      ],
      "metadata": {
        "id": "M3Q09TlTnRpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Act as a Machine Learning engineer who is teaching high school students.\n",
        "<</SYS>>\n",
        "\n",
        "{text}[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "AiWPmrSGnB_-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Explain what are Deep Neural Networks in 2-3 sentences\"\n",
        "print(prompt.format(text=text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZUBwuzwpg_T",
        "outputId": "b140ede9-73c7-43de-8a26-b29123de5ce9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] <<SYS>> \n",
            "Act as a Machine Learning engineer who is teaching high school students.\n",
            "<</SYS>>\n",
            "\n",
            "Explain what are Deep Neural Networks in 2-3 sentences[/INST]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how we can use it\n",
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKzIGBlApus_",
        "outputId": "fd882786-9a5f-4fe2-cf46-dc5c6ae21b89"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hey there, young minds! So, you wanna know about Deep Neural Networks? Well, imagine you have a super powerful computer that can learn and make decisions all on its own, kinda like how your brain works! Deep Neural Networks are like a bunch of these computers working together to solve really tough problems, like recognizing pictures or understanding speech. They're like the ultimate team players when it comes to solving complex tasks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Chain"
      ],
      "metadata": {
        "id": "D9po7s6xp6t6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probably the most important component of LangChain is the Chain class. It's a wrapper around the LLM that allows you to create a chain of actions."
      ],
      "metadata": {
        "id": "jdRmFynfqP4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's how you can use the simplest chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "result = chain.run(text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiTPcja1p761",
        "outputId": "ac7d0641-6926-4ce7-e09f-a5b323e7b536"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hey there, young minds! So, you wanna know about Deep Neural Networks? Well, imagine you have a super powerful computer that can learn and make decisions all on its own, kinda like how your brain works! Deep Neural Networks are like a bunch of these computers working together to solve really tough problems, like recognizing pictures or understanding speech. They're like the ultimate team players when it comes to solving complex tasks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chaining Chains"
      ],
      "metadata": {
        "id": "IdrzIQtQq852"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how we can chain multiple chains together. We'll create a chain that will first explain what are Deep Neural Networks and then give a few examples of practical applications.\n",
        "\n",
        "Let's start by creating the second chain:"
      ],
      "metadata": {
        "id": "_kvKERTTrEWd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EBev-K2eq9qN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}