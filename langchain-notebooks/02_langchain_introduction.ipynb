{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPekDBzWTl5ibfa+jXvhgid",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/genai-research-and-practice/blob/main/langchain-notebooks/02_langchain_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "QWEeJ104N1KO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reference**:\n",
        "\n",
        "https://www.pinecone.io/learn/series/langchain/langchain-intro/"
      ],
      "metadata": {
        "id": "rzVyryHRSYGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain\n",
        "!pip install -qU huggingface_hub"
      ],
      "metadata": {
        "id": "ynkH7v2SN2WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai langchain-google-genai"
      ],
      "metadata": {
        "id": "-qsu-glV6cG-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index\n",
        "!pip install einops accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "GfbYslmnHk6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "t6ocib_cOTwY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "QZYjVozm6iHo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
      ],
      "metadata": {
        "id": "GwLif72AOe24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "oB0HYp2P6oMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "XTc-EOOS6qdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE9mRgIDIncH",
        "outputId": "5436d248-82a4-4313-d944-b52279076818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "j5ORehCO-PEe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt Template"
      ],
      "metadata": {
        "id": "yA3DB-J6PWJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "WlAT26hpPP55"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Liama 2"
      ],
      "metadata": {
        "id": "LooTE5MCYufD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off"
      ],
      "metadata": {
        "id": "ebq_WLIFYZu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline"
      ],
      "metadata": {
        "id": "ITp7gH2zYelZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ],
      "metadata": {
        "id": "4zA_IBzfYe-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "lMuId2zkPpfp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJW0J9yrP25v",
        "outputId": "b7865e1a-8991-40c0-ee23-e28104451897"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The New Orleans Saints.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Asking Multiple Questions\n",
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "res = llm_chain.generate(qs)\n",
        "res"
      ],
      "metadata": {
        "id": "B4H0nNs6QDSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686ddaa4-1b89-4c9c-9bb8-4e53b08687ea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='The New Orleans Saints.')], [Generation(text='\\n6 feet = 72 inches\\n72 inches x 2.54 cm/inch = 183 cm\\n\\nSo, if you are 6 ft 4 inches (193 cm) tall, you are approximately 183 cm (6 ft 4 in) tall.')], [Generation(text='There were only 11 people who have walked on the moon. The Apollo missions, which took place between 1969 and 1972, included astronauts Neil Armstrong, Edwin \"Buzz\" Aldrin, Pete Conrad, Alan Bean, Alan Shepard, Edgar Mitchell, David Scott, James Irwin, John Young, Charles Duke, and Eugene Cernan. None of these astronauts had a twin brother or sister.')], [Generation(text='A blade of grass has zero eyes. Eyes are found in animals, not plants. Plants do not have eyes or any other sensory organs like humans do. They rely on other structures such as leaves and roots to sense their environment and respond accordingly. So, the answer is zero!')]], llm_output=None, run=[RunInfo(run_id=UUID('63db8880-d0dd-4a98-8e09-0a2e5272ad7c')), RunInfo(run_id=UUID('8a6c4d65-9483-41ef-8873-0c66f204b5d5')), RunInfo(run_id=UUID('624eeb28-5ca9-40dd-9628-0c25835309a5')), RunInfo(run_id=UUID('950d7b73-9e27-4b15-97f5-68efd1fc97a4'))])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "id": "KIcw11Fh6NqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1893579-3705-486d-93f5-83ea41fc29a0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The New Orleans Saints won the Super Bowl in the 2010 season.\n",
            "If you are 6 feet 4 inches tall, that is approximately 193 centimeters.\n",
            "There were only 12 people who have walked on the moon, and none of them were numbered or named \"the 12th person.\"\n",
            "Blade of grass has zero eyes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gemini"
      ],
      "metadata": {
        "id": "zuwbWQKd6QRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.2, google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "No1x4yPO6Rkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"What are the usecases of LLMs?\")"
      ],
      "metadata": {
        "id": "wZw9mt4k6-9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "yB_H9QPh_dt6",
        "outputId": "68d173e3-5022-4881-a76d-f45195e6c2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> 1. **Language Generation:** LLMs can generate human-like text, including stories, poems, articles, and even code. This can be used for creative writing, content generation, and even language translation.\n> \n> 2. **Summarization:** LLMs can summarize large amounts of text, such as news articles, research papers, and legal documents. This can be useful for quickly getting the gist of a document or for creating executive summaries.\n> \n> 3. **Question Answering:** LLMs can answer questions in a conversational manner. This can be used for customer service, technical support, and even education.\n> \n> 4. **Chatbots:** LLMs can be used to create chatbots that can interact with users in a natural way. This can be used for customer service, sales, and even entertainment.\n> \n> 5. **Code Generation:** LLMs can generate code in a variety of programming languages. This can be used for automating tasks, creating new applications, and even fixing bugs.\n> \n> 6. **Data Analysis:** LLMs can be used to analyze data and identify patterns and trends. This can be used for market research, financial analysis, and even scientific research.\n> \n> 7. **Machine Translation:** LLMs can be used to translate text from one language to another. This can be used for communication, research, and even business.\n> \n> 8. **Sentiment Analysis:** LLMs can be used to analyze the sentiment of text, such as reviews, social media posts, and news articles. This can be used for market research, product development, and even political analysis.\n> \n> 9. **Text Classification:** LLMs can be used to classify text into different categories, such as spam, news, and marketing. This can be used for content filtering, spam detection, and even sentiment analysis.\n> \n> 10. **Named Entity Recognition:** LLMs can be used to identify named entities in text, such as people, places, and organizations. This can be used for information extraction, data analysis, and even knowledge management."
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"Which NFL team won the Super Bowl in the 2010 season?\")\n",
        "to_markdown(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "epXbdS9e7YXv",
        "outputId": "bcd88999-5da1-45ab-835e-658fc65d453e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> New Orleans Saints"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
        "    \"Who was the 12th person on the moon?\",\n",
        "    # \"How many eyes does a blade of grass have?\"\n",
        "]\n",
        "\n",
        "\n",
        "for q in qs:\n",
        "  res = llm.invoke(q)\n",
        "  print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4znSVXS7Iug",
        "outputId": "ee9470f7-76d3-414f-b81b-ec87385dd22b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Green Bay Packers\n",
            "193.04 centimeters\n",
            "There have only been 12 people to walk on the moon, not 13.\n"
          ]
        }
      ]
    }
  ]
}